2025-01-17 10:58:48,071 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-17 10:58:48,071 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-17 10:58:48,071 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-17 10:58:48,071 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-17 10:58:48,071 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-17 10:58:48,071 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-17 10:58:48,071 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-17 10:58:48,071 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-17 10:58:48,072 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-17 10:58:48,072 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-17 10:58:48,072 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-17 10:58:48,072 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-17 10:58:48,139 - src.functions - DEBUG - Sentences: ['Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', ', hundreds to thousands of instances), especially for classification tasks', 'However, they\ndon’t scale very well to very large datasets, as you will see', 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 'The left plot shows the decision boundaries of three possible linear classifiers', 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 'First, it only works if the data is linearly\nseparable', 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', ', instances that end up in the middle of\nthe street or even on the wrong side)', 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 'With a high value, you get the model on the right', 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 'Then, as usual, you can use the model to make predictions:', 'The first plant is classified as an Iris virginica, while the second is not', 'Let’s look at the scores that the SVM used\nto make these predictions', 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 'This dataset is not linearly separable, as\nyou can see', 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 'You can generate this dataset using the \n function:\n1\n2\n1\n2']
2025-01-17 10:58:48,139 - src.functions - DEBUG - Numbered Sentences: {1: 'Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', 2: ', hundreds to thousands of instances), especially for classification tasks', 3: 'However, they\ndon’t scale very well to very large datasets, as you will see', 4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 5: 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 6: 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 7: 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 8: 'The left plot shows the decision boundaries of three possible linear classifiers', 9: 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 10: 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 11: 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 12: 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 13: 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 14: 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 15: 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 16: 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', 17: ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 19: 'First, it only works if the data is linearly\nseparable', 20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 21: 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 22: 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', 23: ', instances that end up in the middle of\nthe street or even on the wrong side)', 24: 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 25: 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 26: 'With a high value, you get the model on the right', 27: 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 28: 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 29: 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 30: 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 32: 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 33: 'Then, as usual, you can use the model to make predictions:', 34: 'The first plant is classified as an Iris virginica, while the second is not', 35: 'Let’s look at the scores that the SVM used\nto make these predictions', 36: 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 37: 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 38: 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 39: 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 41: 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 42: 'This dataset is not linearly separable, as\nyou can see', 43: 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 44: 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 46: 'You can generate this dataset using the \n function:\n1\n2\n1\n2'}
2025-01-17 10:58:48,288 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-17 10:58:51,199 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-17 10:58:51,202 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    18,
    39
  ],
  "Examples": [
    6,
    20,
    31,
    45
  ],
  "Definitions": [
    1,
    12,
    14,
    19,
    40,
    42
  ]
}
```
2025-01-17 10:58:51,210 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-17 10:58:51,211 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-17 10:58:51,230 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 1
2025-01-17 10:58:51,230 - src.functions - DEBUG - Found rects: [Rect(75.328125, 182.0888671875, 438.5721130371094, 193.1630859375)]
2025-01-17 10:58:51,237 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 1
2025-01-17 10:58:51,237 - src.functions - DEBUG - Found rects: [Rect(443.5721130371094, 182.0888671875, 521.6256103515625, 193.1630859375), Rect(75.328125, 214.1768035888672, 117.83416748046875, 230.00177001953125), Rect(121.76940155029297, 214.1768035888672, 152.46279907226562, 230.00177001953125), Rect(156.39804077148438, 214.1768035888672, 248.49273681640625, 230.00177001953125), Rect(75.328125, 236.5888671875, 363.6017150878906, 247.6630859375)]
2025-01-17 10:58:51,249 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-17 10:58:51,249 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,249 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-17 10:58:51,252 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1
2025-01-17 10:58:51,252 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,252 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1.
2025-01-17 10:58:51,256 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 1
2025-01-17 10:58:51,256 - src.functions - DEBUG - Found rects: [Rect(368.6094055175781, 236.5888671875, 527.1851806640625, 247.6630859375), Rect(75.328125, 249.0888671875, 247.49119567871094, 260.1630859375)]
2025-01-17 10:58:51,262 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1
2025-01-17 10:58:51,262 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,262 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1.
2025-01-17 10:58:51,264 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1
2025-01-17 10:58:51,264 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,264 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1.
2025-01-17 10:58:51,267 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1
2025-01-17 10:58:51,267 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,267 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1.
2025-01-17 10:58:51,270 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-17 10:58:51,270 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-17 10:58:51,279 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 1
2025-01-17 10:58:51,279 - src.functions - DEBUG - Found rects: [Rect(415.2421875, 324.0888671875, 517.8462524414062, 335.1630859375), Rect(75.328125, 336.5888671875, 499.3355712890625, 347.6630859375)]
2025-01-17 10:58:51,285 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 1
2025-01-17 10:58:51,285 - src.functions - DEBUG - Found rects: [Rect(389.6484375, 468.5888671875, 508.47076416015625, 479.6630859375), Rect(75.328125, 481.0888671875, 262.767578125, 492.1630859375)]
2025-01-17 10:58:51,289 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 1
2025-01-17 10:58:51,289 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,289 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 1.
2025-01-17 10:58:51,291 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1
2025-01-17 10:58:51,291 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,291 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1.
2025-01-17 10:58:51,293 - src.functions - DEBUG - Searching for sentence #42: 'This dataset is not linearly separable, as
you can see' on page 1
2025-01-17 10:58:51,293 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,293 - src.functions - WARNING - Could not find sentence 'This dataset is not linearly separable, as
you can see' on page 1.
2025-01-17 10:58:51,301 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-17 10:58:51,301 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,301 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-17 10:58:51,308 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2
2025-01-17 10:58:51,308 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,308 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2.
2025-01-17 10:58:51,315 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2
2025-01-17 10:58:51,315 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,315 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2.
2025-01-17 10:58:51,322 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-17 10:58:51,322 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-17 10:58:51,330 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2
2025-01-17 10:58:51,331 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,331 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2.
2025-01-17 10:58:51,337 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2
2025-01-17 10:58:51,337 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,337 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2.
2025-01-17 10:58:51,344 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 2
2025-01-17 10:58:51,345 - src.functions - DEBUG - Found rects: [Rect(252.51565551757812, 97.5888671875, 522.7468872070312, 108.6630859375), Rect(75.328125, 110.0888671875, 527.3569946289062, 121.1630859375), Rect(75.328125, 122.5888671875, 459.814208984375, 133.6630859375)]
2025-01-17 10:58:51,355 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 2
2025-01-17 10:58:51,355 - src.functions - DEBUG - Found rects: [Rect(75.328125, 522.0888671875, 517.880615234375, 533.1630859375), Rect(75.328125, 535.5888671875, 105.3177261352539, 546.6630859375)]
2025-01-17 10:58:51,363 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2
2025-01-17 10:58:51,364 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,364 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2.
2025-01-17 10:58:51,370 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-17 10:58:51,371 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,371 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-17 10:58:51,377 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2
2025-01-17 10:58:51,377 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,377 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2.
2025-01-17 10:58:51,384 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2
2025-01-17 10:58:51,384 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,384 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2.
2025-01-17 10:58:51,391 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 2
2025-01-17 10:58:51,391 - src.functions - DEBUG - Found rects: [Rect(369.5124206542969, 85.0888671875, 530.3182373046875, 96.1630859375), Rect(75.328125, 97.5888671875, 113.08060455322266, 108.6630859375)]
2025-01-17 10:58:51,400 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2
2025-01-17 10:58:51,400 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,400 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2.
2025-01-17 10:58:51,407 - src.functions - DEBUG - Searching for sentence #42: 'This dataset is not linearly separable, as
you can see' on page 2
2025-01-17 10:58:51,407 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,407 - src.functions - WARNING - Could not find sentence 'This dataset is not linearly separable, as
you can see' on page 2.
2025-01-17 10:58:51,415 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-17 10:58:51,415 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,415 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-17 10:58:51,424 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3
2025-01-17 10:58:51,424 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,424 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3.
2025-01-17 10:58:51,432 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3
2025-01-17 10:58:51,433 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,433 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3.
2025-01-17 10:58:51,441 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-17 10:58:51,441 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,441 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-17 10:58:51,450 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 3
2025-01-17 10:58:51,450 - src.functions - DEBUG - Found rects: [Rect(75.328125, 292.1767883300781, 140.65020751953125, 308.00177001953125), Rect(144.58544921875, 292.1767883300781, 175.27883911132812, 308.00177001953125), Rect(179.214111328125, 292.1767883300781, 271.3088073730469, 308.00177001953125), Rect(75.328125, 314.5888671875, 534.448974609375, 325.6630859375), Rect(75.328125, 327.0888671875, 170.83660888671875, 338.1630859375)]
2025-01-17 10:58:51,463 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3
2025-01-17 10:58:51,463 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,463 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3.
2025-01-17 10:58:51,472 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3
2025-01-17 10:58:51,472 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,472 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3.
2025-01-17 10:58:51,481 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3
2025-01-17 10:58:51,481 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,481 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3.
2025-01-17 10:58:51,490 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 3
2025-01-17 10:58:51,490 - src.functions - DEBUG - Found rects: [Rect(75.328125, 525.5888671875, 521.5821533203125, 536.6630859375), Rect(75.328125, 539.0888671875, 271.3769836425781, 550.1630859375)]
2025-01-17 10:58:51,500 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-17 10:58:51,500 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,500 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-17 10:58:51,509 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3
2025-01-17 10:58:51,509 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,509 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3.
2025-01-17 10:58:51,518 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3
2025-01-17 10:58:51,518 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,518 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3.
2025-01-17 10:58:51,526 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 3
2025-01-17 10:58:51,526 - src.functions - DEBUG - Found rects: []
2025-01-17 10:58:51,526 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 3.
2025-01-17 10:58:51,536 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 3
2025-01-17 10:58:51,536 - src.functions - DEBUG - Found rects: [Rect(175.83595275878906, 327.0888671875, 529.6141357421875, 338.1630859375), Rect(75.328125, 339.5888671875, 444.8943786621094, 350.6630859375)]
2025-01-17 10:58:51,547 - src.functions - DEBUG - Searching for sentence #42: 'This dataset is not linearly separable, as
you can see' on page 3
2025-01-17 10:58:51,547 - src.functions - DEBUG - Found rects: [Rect(368.4375, 352.0888671875, 527.546875, 363.1630859375), Rect(75.328125, 364.5888671875, 121.97260284423828, 375.6630859375)]
2025-01-17 10:58:59,279 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-17 10:58:59,280 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-17 10:58:59,280 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-17 10:58:59,280 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-17 10:58:59,280 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-17 10:58:59,281 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-17 10:58:59,373 - src.functions - DEBUG - Sentences: ['Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', ', hundreds to thousands of instances), especially for classification tasks', 'However, they\ndon’t scale very well to very large datasets, as you will see', 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 'The left plot shows the decision boundaries of three possible linear classifiers', 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 'First, it only works if the data is linearly\nseparable', 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', ', instances that end up in the middle of\nthe street or even on the wrong side)', 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 'With a high value, you get the model on the right', 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 'Then, as usual, you can use the model to make predictions:', 'The first plant is classified as an Iris virginica, while the second is not', 'Let’s look at the scores that the SVM used\nto make these predictions', 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 'This dataset is not linearly separable, as\nyou can see', 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 'You can generate this dataset using the \n function:\n1\n2\n1\n2']
2025-01-17 10:58:59,373 - src.functions - DEBUG - Numbered Sentences: {1: 'Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', 2: ', hundreds to thousands of instances), especially for classification tasks', 3: 'However, they\ndon’t scale very well to very large datasets, as you will see', 4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 5: 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 6: 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 7: 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 8: 'The left plot shows the decision boundaries of three possible linear classifiers', 9: 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 10: 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 11: 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 12: 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 13: 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 14: 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 15: 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 16: 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', 17: ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 19: 'First, it only works if the data is linearly\nseparable', 20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 21: 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 22: 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', 23: ', instances that end up in the middle of\nthe street or even on the wrong side)', 24: 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 25: 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 26: 'With a high value, you get the model on the right', 27: 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 28: 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 29: 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 30: 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 32: 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 33: 'Then, as usual, you can use the model to make predictions:', 34: 'The first plant is classified as an Iris virginica, while the second is not', 35: 'Let’s look at the scores that the SVM used\nto make these predictions', 36: 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 37: 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 38: 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 39: 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 41: 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 42: 'This dataset is not linearly separable, as\nyou can see', 43: 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 44: 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 46: 'You can generate this dataset using the \n function:\n1\n2\n1\n2'}
2025-01-17 10:58:59,376 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-17 10:59:01,746 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-17 10:59:01,749 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    12,
    18,
    39
  ],
  "Examples": [
    6,
    20,
    31,
    45
  ],
  "Definitions": [
    1,
    14,
    18,
    40,
    44
  ]
}
```
2025-01-17 10:59:01,761 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-17 10:59:01,761 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-17 10:59:01,781 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 1
2025-01-17 10:59:01,781 - src.functions - DEBUG - Found rects: [Rect(75.328125, 182.0888671875, 438.5721130371094, 193.1630859375)]
2025-01-17 10:59:01,790 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 1
2025-01-17 10:59:01,790 - src.functions - DEBUG - Found rects: [Rect(443.5721130371094, 182.0888671875, 521.6256103515625, 193.1630859375), Rect(75.328125, 214.1768035888672, 117.83416748046875, 230.00177001953125), Rect(121.76940155029297, 214.1768035888672, 152.46279907226562, 230.00177001953125), Rect(156.39804077148438, 214.1768035888672, 248.49273681640625, 230.00177001953125), Rect(75.328125, 236.5888671875, 363.6017150878906, 247.6630859375)]
2025-01-17 10:59:01,805 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 1
2025-01-17 10:59:01,805 - src.functions - DEBUG - Found rects: [Rect(415.2421875, 324.0888671875, 517.8462524414062, 335.1630859375), Rect(75.328125, 336.5888671875, 499.3355712890625, 347.6630859375)]
2025-01-17 10:59:01,811 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-17 10:59:01,811 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,811 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-17 10:59:01,814 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1
2025-01-17 10:59:01,815 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,815 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1.
2025-01-17 10:59:01,818 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 1
2025-01-17 10:59:01,818 - src.functions - DEBUG - Found rects: [Rect(368.6094055175781, 236.5888671875, 527.1851806640625, 247.6630859375), Rect(75.328125, 249.0888671875, 247.49119567871094, 260.1630859375)]
2025-01-17 10:59:01,823 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1
2025-01-17 10:59:01,823 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,823 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1.
2025-01-17 10:59:01,826 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1
2025-01-17 10:59:01,826 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,826 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1.
2025-01-17 10:59:01,828 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1
2025-01-17 10:59:01,828 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,828 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1.
2025-01-17 10:59:01,831 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-17 10:59:01,831 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-17 10:59:01,840 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 1
2025-01-17 10:59:01,840 - src.functions - DEBUG - Found rects: [Rect(389.6484375, 468.5888671875, 508.47076416015625, 479.6630859375), Rect(75.328125, 481.0888671875, 262.767578125, 492.1630859375)]
2025-01-17 10:59:01,844 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-17 10:59:01,844 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,844 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-17 10:59:01,846 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1
2025-01-17 10:59:01,846 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,846 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1.
2025-01-17 10:59:01,848 - src.functions - DEBUG - Searching for sentence #44: 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 1
2025-01-17 10:59:01,848 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,848 - src.functions - WARNING - Could not find sentence 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 1.
2025-01-17 10:59:01,855 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-17 10:59:01,855 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,855 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-17 10:59:01,862 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2
2025-01-17 10:59:01,862 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,862 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2.
2025-01-17 10:59:01,869 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2
2025-01-17 10:59:01,869 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,869 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2.
2025-01-17 10:59:01,876 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2
2025-01-17 10:59:01,876 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,876 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2.
2025-01-17 10:59:01,883 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-17 10:59:01,883 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-17 10:59:01,891 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2
2025-01-17 10:59:01,891 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,891 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2.
2025-01-17 10:59:01,898 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2
2025-01-17 10:59:01,898 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,898 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2.
2025-01-17 10:59:01,905 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 2
2025-01-17 10:59:01,906 - src.functions - DEBUG - Found rects: [Rect(252.51565551757812, 97.5888671875, 522.7468872070312, 108.6630859375), Rect(75.328125, 110.0888671875, 527.3569946289062, 121.1630859375), Rect(75.328125, 122.5888671875, 459.814208984375, 133.6630859375)]
2025-01-17 10:59:01,916 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 2
2025-01-17 10:59:01,916 - src.functions - DEBUG - Found rects: [Rect(75.328125, 522.0888671875, 517.880615234375, 533.1630859375), Rect(75.328125, 535.5888671875, 105.3177261352539, 546.6630859375)]
2025-01-17 10:59:01,924 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2
2025-01-17 10:59:01,924 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,924 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2.
2025-01-17 10:59:01,931 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-17 10:59:01,931 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,931 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-17 10:59:01,938 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2
2025-01-17 10:59:01,938 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,938 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2.
2025-01-17 10:59:01,945 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-17 10:59:01,945 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-17 10:59:01,954 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2
2025-01-17 10:59:01,954 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,954 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2.
2025-01-17 10:59:01,960 - src.functions - DEBUG - Searching for sentence #44: 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 2
2025-01-17 10:59:01,960 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,960 - src.functions - WARNING - Could not find sentence 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 2.
2025-01-17 10:59:01,969 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-17 10:59:01,969 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,969 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-17 10:59:01,977 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3
2025-01-17 10:59:01,978 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,978 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3.
2025-01-17 10:59:01,986 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3
2025-01-17 10:59:01,986 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,986 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3.
2025-01-17 10:59:01,995 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3
2025-01-17 10:59:01,995 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:01,995 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3.
2025-01-17 10:59:02,003 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-17 10:59:02,003 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:02,003 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-17 10:59:02,012 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 3
2025-01-17 10:59:02,012 - src.functions - DEBUG - Found rects: [Rect(75.328125, 292.1767883300781, 140.65020751953125, 308.00177001953125), Rect(144.58544921875, 292.1767883300781, 175.27883911132812, 308.00177001953125), Rect(179.214111328125, 292.1767883300781, 271.3088073730469, 308.00177001953125), Rect(75.328125, 314.5888671875, 534.448974609375, 325.6630859375), Rect(75.328125, 327.0888671875, 170.83660888671875, 338.1630859375)]
2025-01-17 10:59:02,025 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3
2025-01-17 10:59:02,025 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:02,025 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3.
2025-01-17 10:59:02,034 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3
2025-01-17 10:59:02,034 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:02,034 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3.
2025-01-17 10:59:02,042 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3
2025-01-17 10:59:02,042 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:02,042 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3.
2025-01-17 10:59:02,052 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 3
2025-01-17 10:59:02,052 - src.functions - DEBUG - Found rects: [Rect(75.328125, 525.5888671875, 521.5821533203125, 536.6630859375), Rect(75.328125, 539.0888671875, 271.3769836425781, 550.1630859375)]
2025-01-17 10:59:02,062 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-17 10:59:02,062 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:02,062 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-17 10:59:02,071 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3
2025-01-17 10:59:02,071 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:02,071 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3.
2025-01-17 10:59:02,079 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-17 10:59:02,079 - src.functions - DEBUG - Found rects: []
2025-01-17 10:59:02,079 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-17 10:59:02,090 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 3
2025-01-17 10:59:02,090 - src.functions - DEBUG - Found rects: [Rect(175.83595275878906, 327.0888671875, 529.6141357421875, 338.1630859375), Rect(75.328125, 339.5888671875, 444.8943786621094, 350.6630859375)]
2025-01-17 10:59:02,102 - src.functions - DEBUG - Searching for sentence #44: 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 3
2025-01-17 10:59:02,102 - src.functions - DEBUG - Found rects: [Rect(244.54690551757812, 481.816650390625, 403.197998046875, 490.122314453125), Rect(75.328125, 499.5888671875, 402.28619384765625, 510.6630859375), Rect(75.328125, 513.0888671875, 346.64215087890625, 524.1630859375), Rect(416.6328125, 513.0888671875, 443.0087890625, 524.1630859375), Rect(488.0, 513.0888671875, 526.592041015625, 524.1630859375)]
2025-01-17 11:06:13,299 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-17 11:06:13,300 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-17 11:06:13,300 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-17 11:06:13,300 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-17 11:06:13,300 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-17 11:06:13,300 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-17 11:06:13,424 - src.functions - DEBUG - Sentences: ['Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', ', hundreds to thousands of instances), especially for classification tasks', 'However, they\ndon’t scale very well to very large datasets, as you will see', 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 'The left plot shows the decision boundaries of three possible linear classifiers', 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 'First, it only works if the data is linearly\nseparable', 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', ', instances that end up in the middle of\nthe street or even on the wrong side)', 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 'With a high value, you get the model on the right', 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 'Then, as usual, you can use the model to make predictions:', 'The first plant is classified as an Iris virginica, while the second is not', 'Let’s look at the scores that the SVM used\nto make these predictions', 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 'This dataset is not linearly separable, as\nyou can see', 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 'You can generate this dataset using the \n function:\n1\n2\n1\n2']
2025-01-17 11:06:13,425 - src.functions - DEBUG - Numbered Sentences: {1: 'Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', 2: ', hundreds to thousands of instances), especially for classification tasks', 3: 'However, they\ndon’t scale very well to very large datasets, as you will see', 4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 5: 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 6: 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 7: 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 8: 'The left plot shows the decision boundaries of three possible linear classifiers', 9: 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 10: 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 11: 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 12: 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 13: 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 14: 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 15: 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 16: 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', 17: ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 19: 'First, it only works if the data is linearly\nseparable', 20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 21: 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 22: 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', 23: ', instances that end up in the middle of\nthe street or even on the wrong side)', 24: 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 25: 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 26: 'With a high value, you get the model on the right', 27: 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 28: 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 29: 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 30: 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 32: 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 33: 'Then, as usual, you can use the model to make predictions:', 34: 'The first plant is classified as an Iris virginica, while the second is not', 35: 'Let’s look at the scores that the SVM used\nto make these predictions', 36: 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 37: 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 38: 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 39: 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 41: 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 42: 'This dataset is not linearly separable, as\nyou can see', 43: 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 44: 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 46: 'You can generate this dataset using the \n function:\n1\n2\n1\n2'}
2025-01-17 11:06:13,747 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-17 11:06:17,415 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-17 11:06:17,417 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    18,
    39
  ],
  "Examples": [
    6,
    20,
    31,
    45
  ],
  "Definitions": [
    1,
    12,
    14,
    19,
    40
  ]
}
```
2025-01-17 11:06:17,429 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-17 11:06:17,429 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-17 11:06:17,453 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 1
2025-01-17 11:06:17,453 - src.functions - DEBUG - Found rects: [Rect(75.328125, 182.0888671875, 438.5721130371094, 193.1630859375)]
2025-01-17 11:06:17,461 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 1
2025-01-17 11:06:17,461 - src.functions - DEBUG - Found rects: [Rect(443.5721130371094, 182.0888671875, 521.6256103515625, 193.1630859375), Rect(75.328125, 214.1768035888672, 117.83416748046875, 230.00177001953125), Rect(121.76940155029297, 214.1768035888672, 152.46279907226562, 230.00177001953125), Rect(156.39804077148438, 214.1768035888672, 248.49273681640625, 230.00177001953125), Rect(75.328125, 236.5888671875, 363.6017150878906, 247.6630859375)]
2025-01-17 11:06:17,476 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-17 11:06:17,476 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,476 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-17 11:06:17,479 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1
2025-01-17 11:06:17,479 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,479 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1.
2025-01-17 11:06:17,483 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 1
2025-01-17 11:06:17,483 - src.functions - DEBUG - Found rects: [Rect(368.6094055175781, 236.5888671875, 527.1851806640625, 247.6630859375), Rect(75.328125, 249.0888671875, 247.49119567871094, 260.1630859375)]
2025-01-17 11:06:17,489 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1
2025-01-17 11:06:17,489 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,489 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1.
2025-01-17 11:06:17,492 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1
2025-01-17 11:06:17,492 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,492 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1.
2025-01-17 11:06:17,494 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1
2025-01-17 11:06:17,494 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,494 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1.
2025-01-17 11:06:17,497 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-17 11:06:17,497 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-17 11:06:17,506 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 1
2025-01-17 11:06:17,506 - src.functions - DEBUG - Found rects: [Rect(415.2421875, 324.0888671875, 517.8462524414062, 335.1630859375), Rect(75.328125, 336.5888671875, 499.3355712890625, 347.6630859375)]
2025-01-17 11:06:17,511 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 1
2025-01-17 11:06:17,512 - src.functions - DEBUG - Found rects: [Rect(389.6484375, 468.5888671875, 508.47076416015625, 479.6630859375), Rect(75.328125, 481.0888671875, 262.767578125, 492.1630859375)]
2025-01-17 11:06:17,516 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 1
2025-01-17 11:06:17,516 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,516 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 1.
2025-01-17 11:06:17,517 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1
2025-01-17 11:06:17,517 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,518 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1.
2025-01-17 11:06:17,524 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-17 11:06:17,524 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,524 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-17 11:06:17,531 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2
2025-01-17 11:06:17,531 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,531 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2.
2025-01-17 11:06:17,538 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2
2025-01-17 11:06:17,538 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,538 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2.
2025-01-17 11:06:17,545 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-17 11:06:17,545 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-17 11:06:17,553 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2
2025-01-17 11:06:17,553 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,553 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2.
2025-01-17 11:06:17,560 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2
2025-01-17 11:06:17,560 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,560 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2.
2025-01-17 11:06:17,567 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 2
2025-01-17 11:06:17,567 - src.functions - DEBUG - Found rects: [Rect(252.51565551757812, 97.5888671875, 522.7468872070312, 108.6630859375), Rect(75.328125, 110.0888671875, 527.3569946289062, 121.1630859375), Rect(75.328125, 122.5888671875, 459.814208984375, 133.6630859375)]
2025-01-17 11:06:17,577 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 2
2025-01-17 11:06:17,577 - src.functions - DEBUG - Found rects: [Rect(75.328125, 522.0888671875, 517.880615234375, 533.1630859375), Rect(75.328125, 535.5888671875, 105.3177261352539, 546.6630859375)]
2025-01-17 11:06:17,586 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2
2025-01-17 11:06:17,586 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,586 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2.
2025-01-17 11:06:17,593 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-17 11:06:17,593 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,593 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-17 11:06:17,599 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2
2025-01-17 11:06:17,599 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,599 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2.
2025-01-17 11:06:17,606 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2
2025-01-17 11:06:17,606 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,606 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2.
2025-01-17 11:06:17,613 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 2
2025-01-17 11:06:17,613 - src.functions - DEBUG - Found rects: [Rect(369.5124206542969, 85.0888671875, 530.3182373046875, 96.1630859375), Rect(75.328125, 97.5888671875, 113.08060455322266, 108.6630859375)]
2025-01-17 11:06:17,622 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2
2025-01-17 11:06:17,622 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,622 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2.
2025-01-17 11:06:17,630 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-17 11:06:17,630 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,630 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-17 11:06:17,639 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3
2025-01-17 11:06:17,639 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,639 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3.
2025-01-17 11:06:17,647 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3
2025-01-17 11:06:17,647 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,647 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3.
2025-01-17 11:06:17,656 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-17 11:06:17,656 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,656 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-17 11:06:17,665 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 3
2025-01-17 11:06:17,665 - src.functions - DEBUG - Found rects: [Rect(75.328125, 292.1767883300781, 140.65020751953125, 308.00177001953125), Rect(144.58544921875, 292.1767883300781, 175.27883911132812, 308.00177001953125), Rect(179.214111328125, 292.1767883300781, 271.3088073730469, 308.00177001953125), Rect(75.328125, 314.5888671875, 534.448974609375, 325.6630859375), Rect(75.328125, 327.0888671875, 170.83660888671875, 338.1630859375)]
2025-01-17 11:06:17,677 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3
2025-01-17 11:06:17,677 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,678 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3.
2025-01-17 11:06:17,686 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3
2025-01-17 11:06:17,686 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,686 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3.
2025-01-17 11:06:17,695 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3
2025-01-17 11:06:17,695 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,695 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3.
2025-01-17 11:06:17,704 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 3
2025-01-17 11:06:17,704 - src.functions - DEBUG - Found rects: [Rect(75.328125, 525.5888671875, 521.5821533203125, 536.6630859375), Rect(75.328125, 539.0888671875, 271.3769836425781, 550.1630859375)]
2025-01-17 11:06:17,714 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-17 11:06:17,714 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,714 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-17 11:06:17,723 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3
2025-01-17 11:06:17,723 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,723 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3.
2025-01-17 11:06:17,731 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3
2025-01-17 11:06:17,731 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,731 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3.
2025-01-17 11:06:17,740 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 3
2025-01-17 11:06:17,740 - src.functions - DEBUG - Found rects: []
2025-01-17 11:06:17,740 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 3.
2025-01-17 11:06:17,749 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 3
2025-01-17 11:06:17,749 - src.functions - DEBUG - Found rects: [Rect(175.83595275878906, 327.0888671875, 529.6141357421875, 338.1630859375), Rect(75.328125, 339.5888671875, 444.8943786621094, 350.6630859375)]
2025-01-17 13:14:58,483 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-17 13:14:58,484 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-17 13:14:58,485 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-17 13:14:58,485 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-17 13:14:58,485 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-17 13:14:58,485 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-17 13:14:58,646 - src.functions - DEBUG - Sentences: ['Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', ', hundreds to thousands of instances), especially for classification tasks', 'However, they\ndon’t scale very well to very large datasets, as you will see', 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 'The left plot shows the decision boundaries of three possible linear classifiers', 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 'First, it only works if the data is linearly\nseparable', 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', ', instances that end up in the middle of\nthe street or even on the wrong side)', 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 'With a high value, you get the model on the right', 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 'Then, as usual, you can use the model to make predictions:', 'The first plant is classified as an Iris virginica, while the second is not', 'Let’s look at the scores that the SVM used\nto make these predictions', 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 'This dataset is not linearly separable, as\nyou can see', 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 'You can generate this dataset using the \n function:\n1\n2\n1\n2']
2025-01-17 13:14:58,649 - src.functions - DEBUG - Numbered Sentences: {1: 'Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', 2: ', hundreds to thousands of instances), especially for classification tasks', 3: 'However, they\ndon’t scale very well to very large datasets, as you will see', 4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 5: 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 6: 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 7: 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 8: 'The left plot shows the decision boundaries of three possible linear classifiers', 9: 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 10: 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 11: 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 12: 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 13: 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 14: 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 15: 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 16: 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', 17: ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 19: 'First, it only works if the data is linearly\nseparable', 20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 21: 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 22: 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', 23: ', instances that end up in the middle of\nthe street or even on the wrong side)', 24: 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 25: 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 26: 'With a high value, you get the model on the right', 27: 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 28: 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 29: 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 30: 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 32: 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 33: 'Then, as usual, you can use the model to make predictions:', 34: 'The first plant is classified as an Iris virginica, while the second is not', 35: 'Let’s look at the scores that the SVM used\nto make these predictions', 36: 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 37: 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 38: 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 39: 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 41: 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 42: 'This dataset is not linearly separable, as\nyou can see', 43: 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 44: 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 46: 'You can generate this dataset using the \n function:\n1\n2\n1\n2'}
2025-01-17 13:14:58,978 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-17 13:15:59,413 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-17 13:15:59,415 - src.functions - DEBUG - ChatGPT Output: ```json
{
    "Key Concepts": [
        1,
        4,
        5,
        13,
        18,
        19,
        39,
       40
    ],
    "Examples": [
        6,
        20,
        31,
        45
    ],
    "Definitions": [
        1,
        12,
        14,
        15,
        28
    ]
}
```
2025-01-17 13:15:59,424 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-17 13:15:59,424 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-17 13:15:59,440 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 1
2025-01-17 13:15:59,441 - src.functions - DEBUG - Found rects: [Rect(75.328125, 182.0888671875, 438.5721130371094, 193.1630859375)]
2025-01-17 13:15:59,446 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 1
2025-01-17 13:15:59,446 - src.functions - DEBUG - Found rects: [Rect(443.5721130371094, 182.0888671875, 521.6256103515625, 193.1630859375), Rect(75.328125, 214.1768035888672, 117.83416748046875, 230.00177001953125), Rect(121.76940155029297, 214.1768035888672, 152.46279907226562, 230.00177001953125), Rect(156.39804077148438, 214.1768035888672, 248.49273681640625, 230.00177001953125), Rect(75.328125, 236.5888671875, 363.6017150878906, 247.6630859375)]
2025-01-17 13:15:59,456 - src.functions - DEBUG - Searching for sentence #13: 'Large margin classification
Notice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully
determined (or “supported”) by the instances located on the edge of the street' on page 1
2025-01-17 13:15:59,456 - src.functions - DEBUG - Found rects: [Rect(282.4844055175781, 439.316650390625, 365.2608947753906, 447.622314453125), Rect(75.328125, 456.0888671875, 519.5440673828125, 467.1630859375), Rect(75.328125, 468.5888671875, 384.64239501953125, 479.6630859375)]
2025-01-17 13:15:59,465 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-17 13:15:59,467 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,467 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-17 13:15:59,469 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 1
2025-01-17 13:15:59,469 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,469 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 1.
2025-01-17 13:15:59,471 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1
2025-01-17 13:15:59,471 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,471 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1.
2025-01-17 13:15:59,474 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1
2025-01-17 13:15:59,474 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,474 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1.
2025-01-17 13:15:59,477 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 1
2025-01-17 13:15:59,477 - src.functions - DEBUG - Found rects: [Rect(368.6094055175781, 236.5888671875, 527.1851806640625, 247.6630859375), Rect(75.328125, 249.0888671875, 247.49119567871094, 260.1630859375)]
2025-01-17 13:15:59,481 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1
2025-01-17 13:15:59,481 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,481 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1.
2025-01-17 13:15:59,483 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1
2025-01-17 13:15:59,483 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,484 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1.
2025-01-17 13:15:59,485 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1
2025-01-17 13:15:59,486 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,486 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1.
2025-01-17 13:15:59,488 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-17 13:15:59,488 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-17 13:15:59,497 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 1
2025-01-17 13:15:59,498 - src.functions - DEBUG - Found rects: [Rect(415.2421875, 324.0888671875, 517.8462524414062, 335.1630859375), Rect(75.328125, 336.5888671875, 499.3355712890625, 347.6630859375)]
2025-01-17 13:15:59,503 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 1
2025-01-17 13:15:59,503 - src.functions - DEBUG - Found rects: [Rect(389.6484375, 468.5888671875, 508.47076416015625, 479.6630859375), Rect(75.328125, 481.0888671875, 262.767578125, 492.1630859375)]
2025-01-17 13:15:59,510 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 1
2025-01-17 13:15:59,511 - src.functions - DEBUG - Found rects: [Rect(280.1015625, 515.4517822265625, 331.3959655761719, 526.6181030273438), Rect(94.828125, 532.816650390625, 302.26629638671875, 541.122314453125)]
2025-01-17 13:15:59,515 - src.functions - DEBUG - Searching for sentence #28: 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 1
2025-01-17 13:15:59,515 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,515 - src.functions - WARNING - Could not find sentence 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 1.
2025-01-17 13:15:59,525 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-17 13:15:59,525 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,525 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-17 13:15:59,533 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2
2025-01-17 13:15:59,533 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,533 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2.
2025-01-17 13:15:59,540 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2
2025-01-17 13:15:59,540 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,540 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2.
2025-01-17 13:15:59,549 - src.functions - DEBUG - Searching for sentence #13: 'Large margin classification
Notice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully
determined (or “supported”) by the instances located on the edge of the street' on page 2
2025-01-17 13:15:59,549 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,549 - src.functions - WARNING - Could not find sentence 'Large margin classification
Notice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully
determined (or “supported”) by the instances located on the edge of the street' on page 2.
2025-01-17 13:15:59,559 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-17 13:15:59,559 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-17 13:15:59,571 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 2
2025-01-17 13:15:59,571 - src.functions - DEBUG - Found rects: [Rect(369.5124206542969, 85.0888671875, 530.3182373046875, 96.1630859375), Rect(75.328125, 97.5888671875, 113.08060455322266, 108.6630859375)]
2025-01-17 13:15:59,583 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2
2025-01-17 13:15:59,584 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,584 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2.
2025-01-17 13:15:59,592 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2
2025-01-17 13:15:59,592 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,592 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2.
2025-01-17 13:15:59,600 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2
2025-01-17 13:15:59,601 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,601 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2.
2025-01-17 13:15:59,609 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 2
2025-01-17 13:15:59,609 - src.functions - DEBUG - Found rects: [Rect(252.51565551757812, 97.5888671875, 522.7468872070312, 108.6630859375), Rect(75.328125, 110.0888671875, 527.3569946289062, 121.1630859375), Rect(75.328125, 122.5888671875, 459.814208984375, 133.6630859375)]
2025-01-17 13:15:59,621 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 2
2025-01-17 13:15:59,622 - src.functions - DEBUG - Found rects: [Rect(75.328125, 522.0888671875, 517.880615234375, 533.1630859375), Rect(75.328125, 535.5888671875, 105.3177261352539, 546.6630859375)]
2025-01-17 13:15:59,634 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2
2025-01-17 13:15:59,634 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,634 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2.
2025-01-17 13:15:59,643 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-17 13:15:59,643 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,643 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-17 13:15:59,650 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2
2025-01-17 13:15:59,650 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,650 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2.
2025-01-17 13:15:59,660 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2
2025-01-17 13:15:59,660 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,660 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2.
2025-01-17 13:15:59,667 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 2
2025-01-17 13:15:59,668 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,668 - src.functions - WARNING - Could not find sentence 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 2.
2025-01-17 13:15:59,677 - src.functions - DEBUG - Searching for sentence #28: 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 2
2025-01-17 13:15:59,677 - src.functions - DEBUG - Found rects: [Rect(248.72975158691406, 317.0888671875, 349.5337219238281, 328.1630859375), Rect(354.5234375, 317.0888671875, 515.6038208007812, 328.1630859375), Rect(75.328125, 329.5888671875, 230.57064819335938, 340.6630859375)]
2025-01-17 13:15:59,693 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-17 13:15:59,693 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,693 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-17 13:15:59,702 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3
2025-01-17 13:15:59,702 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,702 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3.
2025-01-17 13:15:59,712 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3
2025-01-17 13:15:59,712 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,712 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3.
2025-01-17 13:15:59,722 - src.functions - DEBUG - Searching for sentence #13: 'Large margin classification
Notice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully
determined (or “supported”) by the instances located on the edge of the street' on page 3
2025-01-17 13:15:59,722 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,722 - src.functions - WARNING - Could not find sentence 'Large margin classification
Notice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully
determined (or “supported”) by the instances located on the edge of the street' on page 3.
2025-01-17 13:15:59,733 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-17 13:15:59,733 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,733 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-17 13:15:59,743 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 3
2025-01-17 13:15:59,743 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,743 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 3.
2025-01-17 13:15:59,754 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 3
2025-01-17 13:15:59,754 - src.functions - DEBUG - Found rects: [Rect(75.328125, 292.1767883300781, 140.65020751953125, 308.00177001953125), Rect(144.58544921875, 292.1767883300781, 175.27883911132812, 308.00177001953125), Rect(179.214111328125, 292.1767883300781, 271.3088073730469, 308.00177001953125), Rect(75.328125, 314.5888671875, 534.448974609375, 325.6630859375), Rect(75.328125, 327.0888671875, 170.83660888671875, 338.1630859375)]
2025-01-17 13:15:59,769 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 3
2025-01-17 13:15:59,769 - src.functions - DEBUG - Found rects: [Rect(175.83595275878906, 327.0888671875, 529.6141357421875, 338.1630859375), Rect(75.328125, 339.5888671875, 444.8943786621094, 350.6630859375)]
2025-01-17 13:15:59,780 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3
2025-01-17 13:15:59,780 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,780 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3.
2025-01-17 13:15:59,792 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3
2025-01-17 13:15:59,792 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,793 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3.
2025-01-17 13:15:59,802 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3
2025-01-17 13:15:59,802 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,802 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3.
2025-01-17 13:15:59,813 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 3
2025-01-17 13:15:59,813 - src.functions - DEBUG - Found rects: [Rect(75.328125, 525.5888671875, 521.5821533203125, 536.6630859375), Rect(75.328125, 539.0888671875, 271.3769836425781, 550.1630859375)]
2025-01-17 13:15:59,827 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-17 13:15:59,828 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,828 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-17 13:15:59,837 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3
2025-01-17 13:15:59,837 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,837 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3.
2025-01-17 13:15:59,846 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3
2025-01-17 13:15:59,846 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,846 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3.
2025-01-17 13:15:59,859 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 3
2025-01-17 13:15:59,859 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,859 - src.functions - WARNING - Could not find sentence 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 3.
2025-01-17 13:15:59,868 - src.functions - DEBUG - Searching for sentence #28: 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 3
2025-01-17 13:15:59,868 - src.functions - DEBUG - Found rects: []
2025-01-17 13:15:59,868 - src.functions - WARNING - Could not find sentence 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 3.
2025-01-17 13:47:29,533 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-17 13:47:29,534 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-17 13:47:29,534 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-17 13:47:29,534 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-17 13:47:29,534 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-17 13:47:29,534 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-17 13:47:29,582 - src.functions - DEBUG - Sentences: ['Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', ', hundreds to thousands of instances), especially for classification tasks', 'However, they\ndon’t scale very well to very large datasets, as you will see', 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 'The left plot shows the decision boundaries of three possible linear classifiers', 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 'First, it only works if the data is linearly\nseparable', 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', ', instances that end up in the middle of\nthe street or even on the wrong side)', 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 'With a high value, you get the model on the right', 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 'Then, as usual, you can use the model to make predictions:', 'The first plant is classified as an Iris virginica, while the second is not', 'Let’s look at the scores that the SVM used\nto make these predictions', 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 'This dataset is not linearly separable, as\nyou can see', 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 'You can generate this dataset using the \n function:\n1\n2\n1\n2']
2025-01-17 13:47:29,583 - src.functions - DEBUG - Numbered Sentences: {1: 'Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', 2: ', hundreds to thousands of instances), especially for classification tasks', 3: 'However, they\ndon’t scale very well to very large datasets, as you will see', 4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 5: 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 6: 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 7: 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 8: 'The left plot shows the decision boundaries of three possible linear classifiers', 9: 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 10: 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 11: 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 12: 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 13: 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 14: 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 15: 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 16: 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', 17: ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 19: 'First, it only works if the data is linearly\nseparable', 20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 21: 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 22: 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', 23: ', instances that end up in the middle of\nthe street or even on the wrong side)', 24: 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 25: 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 26: 'With a high value, you get the model on the right', 27: 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 28: 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 29: 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 30: 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 32: 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 33: 'Then, as usual, you can use the model to make predictions:', 34: 'The first plant is classified as an Iris virginica, while the second is not', 35: 'Let’s look at the scores that the SVM used\nto make these predictions', 36: 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 37: 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 38: 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 39: 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 41: 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 42: 'This dataset is not linearly separable, as\nyou can see', 43: 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 44: 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 46: 'You can generate this dataset using the \n function:\n1\n2\n1\n2'}
2025-01-17 13:47:29,812 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-17 13:47:33,772 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-17 13:47:33,774 - src.functions - DEBUG - ChatGPT Output: ```json
{
    "Key Concepts": [
        1,
        4,
        5,
        39
    ],
    "Examples": [
        6,
        20,
        31,
        45
    ],
    "Definitions": [
        1,
        18,
        14,
        12
    ]
}
```
2025-01-17 13:47:33,784 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-17 13:47:33,785 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-17 13:47:33,805 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 1
2025-01-17 13:47:33,805 - src.functions - DEBUG - Found rects: [Rect(75.328125, 182.0888671875, 438.5721130371094, 193.1630859375)]
2025-01-17 13:47:33,809 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 1
2025-01-17 13:47:33,809 - src.functions - DEBUG - Found rects: [Rect(443.5721130371094, 182.0888671875, 521.6256103515625, 193.1630859375), Rect(75.328125, 214.1768035888672, 117.83416748046875, 230.00177001953125), Rect(121.76940155029297, 214.1768035888672, 152.46279907226562, 230.00177001953125), Rect(156.39804077148438, 214.1768035888672, 248.49273681640625, 230.00177001953125), Rect(75.328125, 236.5888671875, 363.6017150878906, 247.6630859375)]
2025-01-17 13:47:33,818 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1
2025-01-17 13:47:33,818 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:33,818 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1.
2025-01-17 13:47:33,821 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 1
2025-01-17 13:47:33,821 - src.functions - DEBUG - Found rects: [Rect(368.6094055175781, 236.5888671875, 527.1851806640625, 247.6630859375), Rect(75.328125, 249.0888671875, 247.49119567871094, 260.1630859375)]
2025-01-17 13:47:33,827 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1
2025-01-17 13:47:33,827 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:33,827 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1.
2025-01-17 13:47:33,829 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1
2025-01-17 13:47:33,829 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:33,829 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1.
2025-01-17 13:47:33,832 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1
2025-01-17 13:47:33,832 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:33,832 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1.
2025-01-17 13:47:33,835 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-17 13:47:33,835 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-17 13:47:33,845 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-17 13:47:33,845 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:33,845 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-17 13:47:33,848 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 1
2025-01-17 13:47:33,848 - src.functions - DEBUG - Found rects: [Rect(389.6484375, 468.5888671875, 508.47076416015625, 479.6630859375), Rect(75.328125, 481.0888671875, 262.767578125, 492.1630859375)]
2025-01-17 13:47:33,854 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 1
2025-01-17 13:47:33,854 - src.functions - DEBUG - Found rects: [Rect(415.2421875, 324.0888671875, 517.8462524414062, 335.1630859375), Rect(75.328125, 336.5888671875, 499.3355712890625, 347.6630859375)]
2025-01-17 13:47:33,865 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-17 13:47:33,865 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:33,865 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-17 13:47:33,872 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2
2025-01-17 13:47:33,872 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:33,872 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2.
2025-01-17 13:47:33,879 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2
2025-01-17 13:47:33,879 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:33,879 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2.
2025-01-17 13:47:33,886 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2
2025-01-17 13:47:33,886 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:33,886 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2.
2025-01-17 13:47:33,893 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2
2025-01-17 13:47:33,893 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:33,893 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2.
2025-01-17 13:47:33,900 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 2
2025-01-17 13:47:33,900 - src.functions - DEBUG - Found rects: [Rect(252.51565551757812, 97.5888671875, 522.7468872070312, 108.6630859375), Rect(75.328125, 110.0888671875, 527.3569946289062, 121.1630859375), Rect(75.328125, 122.5888671875, 459.814208984375, 133.6630859375)]
2025-01-17 13:47:33,911 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 2
2025-01-17 13:47:33,911 - src.functions - DEBUG - Found rects: [Rect(75.328125, 522.0888671875, 517.880615234375, 533.1630859375), Rect(75.328125, 535.5888671875, 105.3177261352539, 546.6630859375)]
2025-01-17 13:47:33,919 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2
2025-01-17 13:47:33,919 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:33,919 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2.
2025-01-17 13:47:33,926 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-17 13:47:33,926 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:33,926 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-17 13:47:33,933 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-17 13:47:33,933 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-17 13:47:33,942 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2
2025-01-17 13:47:33,942 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:33,942 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2.
2025-01-17 13:47:33,949 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2
2025-01-17 13:47:33,949 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:33,949 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2.
2025-01-17 13:47:33,958 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-17 13:47:33,958 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:33,958 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-17 13:47:33,967 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3
2025-01-17 13:47:33,967 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:33,967 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3.
2025-01-17 13:47:33,975 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3
2025-01-17 13:47:33,975 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:33,976 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3.
2025-01-17 13:47:33,985 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 3
2025-01-17 13:47:33,985 - src.functions - DEBUG - Found rects: [Rect(75.328125, 292.1767883300781, 140.65020751953125, 308.00177001953125), Rect(144.58544921875, 292.1767883300781, 175.27883911132812, 308.00177001953125), Rect(179.214111328125, 292.1767883300781, 271.3088073730469, 308.00177001953125), Rect(75.328125, 314.5888671875, 534.448974609375, 325.6630859375), Rect(75.328125, 327.0888671875, 170.83660888671875, 338.1630859375)]
2025-01-17 13:47:33,998 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3
2025-01-17 13:47:33,998 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:33,998 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3.
2025-01-17 13:47:34,007 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3
2025-01-17 13:47:34,007 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:34,007 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3.
2025-01-17 13:47:34,016 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3
2025-01-17 13:47:34,016 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:34,016 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3.
2025-01-17 13:47:34,026 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 3
2025-01-17 13:47:34,026 - src.functions - DEBUG - Found rects: [Rect(75.328125, 525.5888671875, 521.5821533203125, 536.6630859375), Rect(75.328125, 539.0888671875, 271.3769836425781, 550.1630859375)]
2025-01-17 13:47:34,037 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-17 13:47:34,037 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:34,037 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-17 13:47:34,045 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-17 13:47:34,046 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:34,046 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-17 13:47:34,054 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3
2025-01-17 13:47:34,054 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:34,054 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3.
2025-01-17 13:47:34,063 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3
2025-01-17 13:47:34,063 - src.functions - DEBUG - Found rects: []
2025-01-17 13:47:34,063 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3.
2025-01-17 16:00:36,235 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-17 16:00:36,236 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-17 16:00:36,236 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-17 16:00:36,236 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-17 16:00:36,236 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-17 16:00:36,236 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-17 16:00:36,361 - src.functions - DEBUG - Sentences: ['Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', ', hundreds to thousands of instances), especially for classification tasks', 'However, they\ndon’t scale very well to very large datasets, as you will see', 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 'The left plot shows the decision boundaries of three possible linear classifiers', 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 'First, it only works if the data is linearly\nseparable', 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', ', instances that end up in the middle of\nthe street or even on the wrong side)', 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 'With a high value, you get the model on the right', 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 'Then, as usual, you can use the model to make predictions:', 'The first plant is classified as an Iris virginica, while the second is not', 'Let’s look at the scores that the SVM used\nto make these predictions', 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 'This dataset is not linearly separable, as\nyou can see', 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 'You can generate this dataset using the \n function:\n1\n2\n1\n2']
2025-01-17 16:00:36,372 - src.functions - DEBUG - Numbered Sentences: {1: 'Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', 2: ', hundreds to thousands of instances), especially for classification tasks', 3: 'However, they\ndon’t scale very well to very large datasets, as you will see', 4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 5: 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 6: 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 7: 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 8: 'The left plot shows the decision boundaries of three possible linear classifiers', 9: 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 10: 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 11: 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 12: 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 13: 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 14: 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 15: 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 16: 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', 17: ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 19: 'First, it only works if the data is linearly\nseparable', 20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 21: 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 22: 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', 23: ', instances that end up in the middle of\nthe street or even on the wrong side)', 24: 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 25: 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 26: 'With a high value, you get the model on the right', 27: 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 28: 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 29: 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 30: 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 32: 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 33: 'Then, as usual, you can use the model to make predictions:', 34: 'The first plant is classified as an Iris virginica, while the second is not', 35: 'Let’s look at the scores that the SVM used\nto make these predictions', 36: 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 37: 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 38: 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 39: 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 41: 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 42: 'This dataset is not linearly separable, as\nyou can see', 43: 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 44: 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 46: 'You can generate this dataset using the \n function:\n1\n2\n1\n2'}
2025-01-17 16:00:36,698 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-17 16:00:40,539 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-17 16:00:40,542 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    12,
    18,
    21,
    39
  ],
  "Examples": [
    6,
    20,
    31,
    45
  ],
  "Definitions": [
    1,
    14,
    18,
    39,
    40,
     42
  ]
}
```
2025-01-17 16:00:40,552 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-17 16:00:40,553 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-17 16:00:40,577 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 1
2025-01-17 16:00:40,577 - src.functions - DEBUG - Found rects: [Rect(75.328125, 182.0888671875, 438.5721130371094, 193.1630859375)]
2025-01-17 16:00:40,585 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 1
2025-01-17 16:00:40,585 - src.functions - DEBUG - Found rects: [Rect(443.5721130371094, 182.0888671875, 521.6256103515625, 193.1630859375), Rect(75.328125, 214.1768035888672, 117.83416748046875, 230.00177001953125), Rect(121.76940155029297, 214.1768035888672, 152.46279907226562, 230.00177001953125), Rect(156.39804077148438, 214.1768035888672, 248.49273681640625, 230.00177001953125), Rect(75.328125, 236.5888671875, 363.6017150878906, 247.6630859375)]
2025-01-17 16:00:40,599 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 1
2025-01-17 16:00:40,599 - src.functions - DEBUG - Found rects: [Rect(415.2421875, 324.0888671875, 517.8462524414062, 335.1630859375), Rect(75.328125, 336.5888671875, 499.3355712890625, 347.6630859375)]
2025-01-17 16:00:40,605 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-17 16:00:40,606 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,606 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-17 16:00:40,608 - src.functions - DEBUG - Searching for sentence #21: 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 1
2025-01-17 16:00:40,608 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,608 - src.functions - WARNING - Could not find sentence 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 1.
2025-01-17 16:00:40,611 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1
2025-01-17 16:00:40,611 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,611 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1.
2025-01-17 16:00:40,614 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 1
2025-01-17 16:00:40,614 - src.functions - DEBUG - Found rects: [Rect(368.6094055175781, 236.5888671875, 527.1851806640625, 247.6630859375), Rect(75.328125, 249.0888671875, 247.49119567871094, 260.1630859375)]
2025-01-17 16:00:40,619 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1
2025-01-17 16:00:40,619 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,619 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1.
2025-01-17 16:00:40,621 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1
2025-01-17 16:00:40,622 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,622 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1.
2025-01-17 16:00:40,624 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1
2025-01-17 16:00:40,624 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,624 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1.
2025-01-17 16:00:40,626 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-17 16:00:40,626 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-17 16:00:40,634 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 1
2025-01-17 16:00:40,634 - src.functions - DEBUG - Found rects: [Rect(389.6484375, 468.5888671875, 508.47076416015625, 479.6630859375), Rect(75.328125, 481.0888671875, 262.767578125, 492.1630859375)]
2025-01-17 16:00:40,638 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-17 16:00:40,638 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,638 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-17 16:00:40,640 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1
2025-01-17 16:00:40,640 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,640 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1.
2025-01-17 16:00:40,642 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1
2025-01-17 16:00:40,642 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,642 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1.
2025-01-17 16:00:40,644 - src.functions - DEBUG - Searching for sentence #42: 'This dataset is not linearly separable, as
you can see' on page 1
2025-01-17 16:00:40,644 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,644 - src.functions - WARNING - Could not find sentence 'This dataset is not linearly separable, as
you can see' on page 1.
2025-01-17 16:00:40,651 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-17 16:00:40,651 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,651 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-17 16:00:40,658 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2
2025-01-17 16:00:40,658 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,658 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2.
2025-01-17 16:00:40,664 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2
2025-01-17 16:00:40,664 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,664 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2.
2025-01-17 16:00:40,671 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2
2025-01-17 16:00:40,671 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,671 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2.
2025-01-17 16:00:40,678 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-17 16:00:40,678 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-17 16:00:40,687 - src.functions - DEBUG - Searching for sentence #21: 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 2
2025-01-17 16:00:40,687 - src.functions - DEBUG - Found rects: [Rect(272.28125, 217.316650390625, 375.4692077636719, 225.622314453125), Rect(75.328125, 234.0888671875, 314.55462646484375, 245.1630859375)]
2025-01-17 16:00:40,695 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2
2025-01-17 16:00:40,695 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,695 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2.
2025-01-17 16:00:40,702 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2
2025-01-17 16:00:40,702 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,702 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2.
2025-01-17 16:00:40,709 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 2
2025-01-17 16:00:40,709 - src.functions - DEBUG - Found rects: [Rect(252.51565551757812, 97.5888671875, 522.7468872070312, 108.6630859375), Rect(75.328125, 110.0888671875, 527.3569946289062, 121.1630859375), Rect(75.328125, 122.5888671875, 459.814208984375, 133.6630859375)]
2025-01-17 16:00:40,719 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 2
2025-01-17 16:00:40,719 - src.functions - DEBUG - Found rects: [Rect(75.328125, 522.0888671875, 517.880615234375, 533.1630859375), Rect(75.328125, 535.5888671875, 105.3177261352539, 546.6630859375)]
2025-01-17 16:00:40,728 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2
2025-01-17 16:00:40,728 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,728 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2.
2025-01-17 16:00:40,735 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-17 16:00:40,735 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,735 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-17 16:00:40,741 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2
2025-01-17 16:00:40,741 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,742 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2.
2025-01-17 16:00:40,748 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-17 16:00:40,748 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-17 16:00:40,757 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2
2025-01-17 16:00:40,757 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,757 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2.
2025-01-17 16:00:40,764 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2
2025-01-17 16:00:40,764 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,764 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2.
2025-01-17 16:00:40,771 - src.functions - DEBUG - Searching for sentence #42: 'This dataset is not linearly separable, as
you can see' on page 2
2025-01-17 16:00:40,771 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,771 - src.functions - WARNING - Could not find sentence 'This dataset is not linearly separable, as
you can see' on page 2.
2025-01-17 16:00:40,779 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-17 16:00:40,779 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,779 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-17 16:00:40,788 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3
2025-01-17 16:00:40,788 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,788 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3.
2025-01-17 16:00:40,796 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3
2025-01-17 16:00:40,796 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,796 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3.
2025-01-17 16:00:40,805 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3
2025-01-17 16:00:40,805 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,805 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3.
2025-01-17 16:00:40,813 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-17 16:00:40,813 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,813 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-17 16:00:40,822 - src.functions - DEBUG - Searching for sentence #21: 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 3
2025-01-17 16:00:40,822 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,822 - src.functions - WARNING - Could not find sentence 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 3.
2025-01-17 16:00:40,831 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 3
2025-01-17 16:00:40,831 - src.functions - DEBUG - Found rects: [Rect(75.328125, 292.1767883300781, 140.65020751953125, 308.00177001953125), Rect(144.58544921875, 292.1767883300781, 175.27883911132812, 308.00177001953125), Rect(179.214111328125, 292.1767883300781, 271.3088073730469, 308.00177001953125), Rect(75.328125, 314.5888671875, 534.448974609375, 325.6630859375), Rect(75.328125, 327.0888671875, 170.83660888671875, 338.1630859375)]
2025-01-17 16:00:40,844 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3
2025-01-17 16:00:40,844 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,844 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3.
2025-01-17 16:00:40,852 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3
2025-01-17 16:00:40,852 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,852 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3.
2025-01-17 16:00:40,861 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3
2025-01-17 16:00:40,861 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,861 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3.
2025-01-17 16:00:40,870 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 3
2025-01-17 16:00:40,870 - src.functions - DEBUG - Found rects: [Rect(75.328125, 525.5888671875, 521.5821533203125, 536.6630859375), Rect(75.328125, 539.0888671875, 271.3769836425781, 550.1630859375)]
2025-01-17 16:00:40,880 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-17 16:00:40,881 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,881 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-17 16:00:40,889 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3
2025-01-17 16:00:40,889 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,889 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3.
2025-01-17 16:00:40,898 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-17 16:00:40,898 - src.functions - DEBUG - Found rects: []
2025-01-17 16:00:40,898 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-17 16:00:40,907 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 3
2025-01-17 16:00:40,907 - src.functions - DEBUG - Found rects: [Rect(75.328125, 292.1767883300781, 140.65020751953125, 308.00177001953125), Rect(144.58544921875, 292.1767883300781, 175.27883911132812, 308.00177001953125), Rect(179.214111328125, 292.1767883300781, 271.3088073730469, 308.00177001953125), Rect(75.328125, 314.5888671875, 534.448974609375, 325.6630859375), Rect(75.328125, 327.0888671875, 170.83660888671875, 338.1630859375)]
2025-01-17 16:00:40,920 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 3
2025-01-17 16:00:40,920 - src.functions - DEBUG - Found rects: [Rect(175.83595275878906, 327.0888671875, 529.6141357421875, 338.1630859375), Rect(75.328125, 339.5888671875, 444.8943786621094, 350.6630859375)]
2025-01-17 16:00:40,931 - src.functions - DEBUG - Searching for sentence #42: 'This dataset is not linearly separable, as
you can see' on page 3
2025-01-17 16:00:40,931 - src.functions - DEBUG - Found rects: [Rect(368.4375, 352.0888671875, 527.546875, 363.1630859375), Rect(75.328125, 364.5888671875, 121.97260284423828, 375.6630859375)]
2025-01-17 21:09:25,932 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-17 21:09:25,932 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-17 21:09:25,933 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-17 21:09:25,933 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-17 21:09:25,933 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-17 21:09:25,934 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-17 21:09:25,982 - src.functions - DEBUG - Sentences: ['Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', ', hundreds to thousands of instances), especially for classification tasks', 'However, they\ndon’t scale very well to very large datasets, as you will see', 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 'The left plot shows the decision boundaries of three possible linear classifiers', 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 'First, it only works if the data is linearly\nseparable', 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', ', instances that end up in the middle of\nthe street or even on the wrong side)', 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 'With a high value, you get the model on the right', 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 'Then, as usual, you can use the model to make predictions:', 'The first plant is classified as an Iris virginica, while the second is not', 'Let’s look at the scores that the SVM used\nto make these predictions', 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 'This dataset is not linearly separable, as\nyou can see', 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 'You can generate this dataset using the \n function:\n1\n2\n1\n2']
2025-01-17 21:09:25,982 - src.functions - DEBUG - Numbered Sentences: {1: 'Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', 2: ', hundreds to thousands of instances), especially for classification tasks', 3: 'However, they\ndon’t scale very well to very large datasets, as you will see', 4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 5: 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 6: 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 7: 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 8: 'The left plot shows the decision boundaries of three possible linear classifiers', 9: 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 10: 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 11: 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 12: 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 13: 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 14: 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 15: 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 16: 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', 17: ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 19: 'First, it only works if the data is linearly\nseparable', 20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 21: 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 22: 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', 23: ', instances that end up in the middle of\nthe street or even on the wrong side)', 24: 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 25: 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 26: 'With a high value, you get the model on the right', 27: 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 28: 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 29: 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 30: 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 32: 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 33: 'Then, as usual, you can use the model to make predictions:', 34: 'The first plant is classified as an Iris virginica, while the second is not', 35: 'Let’s look at the scores that the SVM used\nto make these predictions', 36: 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 37: 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 38: 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 39: 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 41: 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 42: 'This dataset is not linearly separable, as\nyou can see', 43: 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 44: 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 46: 'You can generate this dataset using the \n function:\n1\n2\n1\n2'}
2025-01-17 21:09:26,448 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-17 21:09:30,288 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-17 21:09:30,291 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    12,
    18,
    21,
    39
  ],
  "Examples": [
    6,
    20,
    31,
    45
  ],
  "Definitions": [
    1,
    13,
    14,
    15,
    19,
    28,
    40,
    44
  ]
}
```
2025-01-17 21:09:30,302 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-17 21:09:30,302 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-17 21:09:30,326 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 1
2025-01-17 21:09:30,326 - src.functions - DEBUG - Found rects: [Rect(75.328125, 182.0888671875, 438.5721130371094, 193.1630859375)]
2025-01-17 21:09:30,333 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 1
2025-01-17 21:09:30,333 - src.functions - DEBUG - Found rects: [Rect(443.5721130371094, 182.0888671875, 521.6256103515625, 193.1630859375), Rect(75.328125, 214.1768035888672, 117.83416748046875, 230.00177001953125), Rect(121.76940155029297, 214.1768035888672, 152.46279907226562, 230.00177001953125), Rect(156.39804077148438, 214.1768035888672, 248.49273681640625, 230.00177001953125), Rect(75.328125, 236.5888671875, 363.6017150878906, 247.6630859375)]
2025-01-17 21:09:30,347 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 1
2025-01-17 21:09:30,347 - src.functions - DEBUG - Found rects: [Rect(415.2421875, 324.0888671875, 517.8462524414062, 335.1630859375), Rect(75.328125, 336.5888671875, 499.3355712890625, 347.6630859375)]
2025-01-17 21:09:30,354 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-17 21:09:30,354 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,354 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-17 21:09:30,357 - src.functions - DEBUG - Searching for sentence #21: 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 1
2025-01-17 21:09:30,357 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,357 - src.functions - WARNING - Could not find sentence 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 1.
2025-01-17 21:09:30,360 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1
2025-01-17 21:09:30,360 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,360 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1.
2025-01-17 21:09:30,363 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 1
2025-01-17 21:09:30,363 - src.functions - DEBUG - Found rects: [Rect(368.6094055175781, 236.5888671875, 527.1851806640625, 247.6630859375), Rect(75.328125, 249.0888671875, 247.49119567871094, 260.1630859375)]
2025-01-17 21:09:30,368 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1
2025-01-17 21:09:30,368 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,368 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1.
2025-01-17 21:09:30,371 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1
2025-01-17 21:09:30,371 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,371 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1.
2025-01-17 21:09:30,373 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1
2025-01-17 21:09:30,373 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,373 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1.
2025-01-17 21:09:30,375 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-17 21:09:30,375 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-17 21:09:30,384 - src.functions - DEBUG - Searching for sentence #13: 'Large margin classification
Notice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully
determined (or “supported”) by the instances located on the edge of the street' on page 1
2025-01-17 21:09:30,384 - src.functions - DEBUG - Found rects: [Rect(282.4844055175781, 439.316650390625, 365.2608947753906, 447.622314453125), Rect(75.328125, 456.0888671875, 519.5440673828125, 467.1630859375), Rect(75.328125, 468.5888671875, 384.64239501953125, 479.6630859375)]
2025-01-17 21:09:30,390 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 1
2025-01-17 21:09:30,390 - src.functions - DEBUG - Found rects: [Rect(389.6484375, 468.5888671875, 508.47076416015625, 479.6630859375), Rect(75.328125, 481.0888671875, 262.767578125, 492.1630859375)]
2025-01-17 21:09:30,394 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 1
2025-01-17 21:09:30,394 - src.functions - DEBUG - Found rects: [Rect(280.1015625, 515.4517822265625, 331.3959655761719, 526.6181030273438), Rect(94.828125, 532.816650390625, 302.26629638671875, 541.122314453125)]
2025-01-17 21:09:30,398 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 1
2025-01-17 21:09:30,398 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,398 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 1.
2025-01-17 21:09:30,400 - src.functions - DEBUG - Searching for sentence #28: 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 1
2025-01-17 21:09:30,400 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,400 - src.functions - WARNING - Could not find sentence 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 1.
2025-01-17 21:09:30,402 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1
2025-01-17 21:09:30,402 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,402 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1.
2025-01-17 21:09:30,404 - src.functions - DEBUG - Searching for sentence #44: 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 1
2025-01-17 21:09:30,404 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,404 - src.functions - WARNING - Could not find sentence 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 1.
2025-01-17 21:09:30,411 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-17 21:09:30,411 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,411 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-17 21:09:30,417 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2
2025-01-17 21:09:30,417 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,418 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2.
2025-01-17 21:09:30,424 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2
2025-01-17 21:09:30,424 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,424 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2.
2025-01-17 21:09:30,431 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2
2025-01-17 21:09:30,431 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,431 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2.
2025-01-17 21:09:30,438 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-17 21:09:30,438 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-17 21:09:30,447 - src.functions - DEBUG - Searching for sentence #21: 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 2
2025-01-17 21:09:30,447 - src.functions - DEBUG - Found rects: [Rect(272.28125, 217.316650390625, 375.4692077636719, 225.622314453125), Rect(75.328125, 234.0888671875, 314.55462646484375, 245.1630859375)]
2025-01-17 21:09:30,456 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2
2025-01-17 21:09:30,456 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,456 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2.
2025-01-17 21:09:30,463 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2
2025-01-17 21:09:30,463 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,463 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2.
2025-01-17 21:09:30,470 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 2
2025-01-17 21:09:30,470 - src.functions - DEBUG - Found rects: [Rect(252.51565551757812, 97.5888671875, 522.7468872070312, 108.6630859375), Rect(75.328125, 110.0888671875, 527.3569946289062, 121.1630859375), Rect(75.328125, 122.5888671875, 459.814208984375, 133.6630859375)]
2025-01-17 21:09:30,481 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 2
2025-01-17 21:09:30,481 - src.functions - DEBUG - Found rects: [Rect(75.328125, 522.0888671875, 517.880615234375, 533.1630859375), Rect(75.328125, 535.5888671875, 105.3177261352539, 546.6630859375)]
2025-01-17 21:09:30,489 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2
2025-01-17 21:09:30,489 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,489 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2.
2025-01-17 21:09:30,496 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-17 21:09:30,496 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,496 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-17 21:09:30,503 - src.functions - DEBUG - Searching for sentence #13: 'Large margin classification
Notice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully
determined (or “supported”) by the instances located on the edge of the street' on page 2
2025-01-17 21:09:30,503 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,503 - src.functions - WARNING - Could not find sentence 'Large margin classification
Notice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully
determined (or “supported”) by the instances located on the edge of the street' on page 2.
2025-01-17 21:09:30,510 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2
2025-01-17 21:09:30,510 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,510 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2.
2025-01-17 21:09:30,517 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 2
2025-01-17 21:09:30,517 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,517 - src.functions - WARNING - Could not find sentence 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 2.
2025-01-17 21:09:30,523 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 2
2025-01-17 21:09:30,523 - src.functions - DEBUG - Found rects: [Rect(369.5124206542969, 85.0888671875, 530.3182373046875, 96.1630859375), Rect(75.328125, 97.5888671875, 113.08060455322266, 108.6630859375)]
2025-01-17 21:09:30,533 - src.functions - DEBUG - Searching for sentence #28: 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 2
2025-01-17 21:09:30,533 - src.functions - DEBUG - Found rects: [Rect(248.72975158691406, 317.0888671875, 349.5337219238281, 328.1630859375), Rect(354.5234375, 317.0888671875, 515.6038208007812, 328.1630859375), Rect(75.328125, 329.5888671875, 230.57064819335938, 340.6630859375)]
2025-01-17 21:09:30,542 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2
2025-01-17 21:09:30,542 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,542 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2.
2025-01-17 21:09:30,549 - src.functions - DEBUG - Searching for sentence #44: 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 2
2025-01-17 21:09:30,549 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,549 - src.functions - WARNING - Could not find sentence 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 2.
2025-01-17 21:09:30,558 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-17 21:09:30,558 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,558 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-17 21:09:30,566 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3
2025-01-17 21:09:30,566 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,566 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3.
2025-01-17 21:09:30,575 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3
2025-01-17 21:09:30,575 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,575 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3.
2025-01-17 21:09:30,583 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3
2025-01-17 21:09:30,583 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,583 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3.
2025-01-17 21:09:30,591 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-17 21:09:30,591 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,591 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-17 21:09:30,600 - src.functions - DEBUG - Searching for sentence #21: 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 3
2025-01-17 21:09:30,600 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,600 - src.functions - WARNING - Could not find sentence 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 3.
2025-01-17 21:09:30,609 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 3
2025-01-17 21:09:30,609 - src.functions - DEBUG - Found rects: [Rect(75.328125, 292.1767883300781, 140.65020751953125, 308.00177001953125), Rect(144.58544921875, 292.1767883300781, 175.27883911132812, 308.00177001953125), Rect(179.214111328125, 292.1767883300781, 271.3088073730469, 308.00177001953125), Rect(75.328125, 314.5888671875, 534.448974609375, 325.6630859375), Rect(75.328125, 327.0888671875, 170.83660888671875, 338.1630859375)]
2025-01-17 21:09:30,622 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3
2025-01-17 21:09:30,622 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,622 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3.
2025-01-17 21:09:30,631 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3
2025-01-17 21:09:30,631 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,631 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3.
2025-01-17 21:09:30,640 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3
2025-01-17 21:09:30,640 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,640 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3.
2025-01-17 21:09:30,649 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 3
2025-01-17 21:09:30,649 - src.functions - DEBUG - Found rects: [Rect(75.328125, 525.5888671875, 521.5821533203125, 536.6630859375), Rect(75.328125, 539.0888671875, 271.3769836425781, 550.1630859375)]
2025-01-17 21:09:30,659 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-17 21:09:30,659 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,659 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-17 21:09:30,668 - src.functions - DEBUG - Searching for sentence #13: 'Large margin classification
Notice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully
determined (or “supported”) by the instances located on the edge of the street' on page 3
2025-01-17 21:09:30,668 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,668 - src.functions - WARNING - Could not find sentence 'Large margin classification
Notice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully
determined (or “supported”) by the instances located on the edge of the street' on page 3.
2025-01-17 21:09:30,676 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3
2025-01-17 21:09:30,676 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,676 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3.
2025-01-17 21:09:30,685 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 3
2025-01-17 21:09:30,685 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,685 - src.functions - WARNING - Could not find sentence 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 3.
2025-01-17 21:09:30,693 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 3
2025-01-17 21:09:30,693 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,693 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 3.
2025-01-17 21:09:30,702 - src.functions - DEBUG - Searching for sentence #28: 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 3
2025-01-17 21:09:30,702 - src.functions - DEBUG - Found rects: []
2025-01-17 21:09:30,702 - src.functions - WARNING - Could not find sentence 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 3.
2025-01-17 21:09:30,711 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 3
2025-01-17 21:09:30,711 - src.functions - DEBUG - Found rects: [Rect(175.83595275878906, 327.0888671875, 529.6141357421875, 338.1630859375), Rect(75.328125, 339.5888671875, 444.8943786621094, 350.6630859375)]
2025-01-17 21:09:30,722 - src.functions - DEBUG - Searching for sentence #44: 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 3
2025-01-17 21:09:30,722 - src.functions - DEBUG - Found rects: [Rect(244.54690551757812, 481.816650390625, 403.197998046875, 490.122314453125), Rect(75.328125, 499.5888671875, 402.28619384765625, 510.6630859375), Rect(75.328125, 513.0888671875, 346.64215087890625, 524.1630859375), Rect(416.6328125, 513.0888671875, 443.0087890625, 524.1630859375), Rect(488.0, 513.0888671875, 526.592041015625, 524.1630859375)]
2025-01-18 01:31:19,684 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-18 01:31:19,697 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-18 01:31:19,700 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-18 01:31:19,721 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-18 01:31:19,721 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-18 01:31:19,721 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-18 01:31:19,825 - src.functions - DEBUG - Sentences: ['Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', ', hundreds to thousands of instances), especially for classification tasks', 'However, they\ndon’t scale very well to very large datasets, as you will see', 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 'The left plot shows the decision boundaries of three possible linear classifiers', 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 'First, it only works if the data is linearly\nseparable', 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', ', instances that end up in the middle of\nthe street or even on the wrong side)', 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 'With a high value, you get the model on the right', 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 'Then, as usual, you can use the model to make predictions:', 'The first plant is classified as an Iris virginica, while the second is not', 'Let’s look at the scores that the SVM used\nto make these predictions', 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 'This dataset is not linearly separable, as\nyou can see', 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 'You can generate this dataset using the \n function:\n1\n2\n1\n2']
2025-01-18 01:31:19,828 - src.functions - DEBUG - Numbered Sentences: {1: 'Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', 2: ', hundreds to thousands of instances), especially for classification tasks', 3: 'However, they\ndon’t scale very well to very large datasets, as you will see', 4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 5: 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 6: 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 7: 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 8: 'The left plot shows the decision boundaries of three possible linear classifiers', 9: 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 10: 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 11: 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 12: 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 13: 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 14: 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 15: 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 16: 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', 17: ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 19: 'First, it only works if the data is linearly\nseparable', 20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 21: 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 22: 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', 23: ', instances that end up in the middle of\nthe street or even on the wrong side)', 24: 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 25: 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 26: 'With a high value, you get the model on the right', 27: 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 28: 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 29: 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 30: 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 32: 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 33: 'Then, as usual, you can use the model to make predictions:', 34: 'The first plant is classified as an Iris virginica, while the second is not', 35: 'Let’s look at the scores that the SVM used\nto make these predictions', 36: 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 37: 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 38: 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 39: 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 41: 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 42: 'This dataset is not linearly separable, as\nyou can see', 43: 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 44: 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 46: 'You can generate this dataset using the \n function:\n1\n2\n1\n2'}
2025-01-18 01:31:20,174 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-18 01:31:25,817 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-18 01:31:25,820 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    11,
    18,
    39
  ],
  "Examples": [
    6,
    20,
    31,
    45
  ],
  "Definitions": [
    1,
    12,
    14,
    15,
    19,
    40
  ]
}
```
2025-01-18 01:31:25,832 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-18 01:31:25,832 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-18 01:31:25,856 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 1
2025-01-18 01:31:25,856 - src.functions - DEBUG - Found rects: [Rect(75.328125, 182.0888671875, 438.5721130371094, 193.1630859375)]
2025-01-18 01:31:25,864 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 1
2025-01-18 01:31:25,864 - src.functions - DEBUG - Found rects: [Rect(443.5721130371094, 182.0888671875, 521.6256103515625, 193.1630859375), Rect(75.328125, 214.1768035888672, 117.83416748046875, 230.00177001953125), Rect(121.76940155029297, 214.1768035888672, 152.46279907226562, 230.00177001953125), Rect(156.39804077148438, 214.1768035888672, 248.49273681640625, 230.00177001953125), Rect(75.328125, 236.5888671875, 363.6017150878906, 247.6630859375)]
2025-01-18 01:31:25,878 - src.functions - DEBUG - Searching for sentence #11: 'In contrast, the solid
line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the
two classes but also stays as far away from the closest training instances as possible' on page 1
2025-01-18 01:31:25,878 - src.functions - DEBUG - Found rects: [Rect(444.39410400390625, 299.0888671875, 526.0419921875, 310.1630859375), Rect(75.328125, 311.5888671875, 527.990478515625, 322.6630859375), Rect(75.328125, 324.0888671875, 410.24267578125, 335.1630859375)]
2025-01-18 01:31:25,886 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-18 01:31:25,886 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:25,886 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-18 01:31:25,889 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1
2025-01-18 01:31:25,889 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:25,889 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1.
2025-01-18 01:31:25,892 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 1
2025-01-18 01:31:25,892 - src.functions - DEBUG - Found rects: [Rect(368.6094055175781, 236.5888671875, 527.1851806640625, 247.6630859375), Rect(75.328125, 249.0888671875, 247.49119567871094, 260.1630859375)]
2025-01-18 01:31:25,897 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1
2025-01-18 01:31:25,897 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:25,897 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1.
2025-01-18 01:31:25,899 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1
2025-01-18 01:31:25,899 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:25,899 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1.
2025-01-18 01:31:25,902 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1
2025-01-18 01:31:25,902 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:25,902 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1.
2025-01-18 01:31:25,904 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-18 01:31:25,904 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-18 01:31:25,912 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 1
2025-01-18 01:31:25,913 - src.functions - DEBUG - Found rects: [Rect(415.2421875, 324.0888671875, 517.8462524414062, 335.1630859375), Rect(75.328125, 336.5888671875, 499.3355712890625, 347.6630859375)]
2025-01-18 01:31:25,917 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 1
2025-01-18 01:31:25,917 - src.functions - DEBUG - Found rects: [Rect(389.6484375, 468.5888671875, 508.47076416015625, 479.6630859375), Rect(75.328125, 481.0888671875, 262.767578125, 492.1630859375)]
2025-01-18 01:31:25,922 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 1
2025-01-18 01:31:25,922 - src.functions - DEBUG - Found rects: [Rect(280.1015625, 515.4517822265625, 331.3959655761719, 526.6181030273438), Rect(94.828125, 532.816650390625, 302.26629638671875, 541.122314453125)]
2025-01-18 01:31:25,926 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 1
2025-01-18 01:31:25,926 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:25,926 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 1.
2025-01-18 01:31:25,928 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1
2025-01-18 01:31:25,928 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:25,928 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1.
2025-01-18 01:31:25,934 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-18 01:31:25,934 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:25,934 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-18 01:31:25,941 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2
2025-01-18 01:31:25,941 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:25,941 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2.
2025-01-18 01:31:25,948 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2
2025-01-18 01:31:25,948 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:25,948 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2.
2025-01-18 01:31:25,955 - src.functions - DEBUG - Searching for sentence #11: 'In contrast, the solid
line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the
two classes but also stays as far away from the closest training instances as possible' on page 2
2025-01-18 01:31:25,955 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:25,955 - src.functions - WARNING - Could not find sentence 'In contrast, the solid
line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the
two classes but also stays as far away from the closest training instances as possible' on page 2.
2025-01-18 01:31:25,961 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-18 01:31:25,961 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-18 01:31:25,970 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2
2025-01-18 01:31:25,970 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:25,970 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2.
2025-01-18 01:31:25,977 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2
2025-01-18 01:31:25,977 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:25,977 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2.
2025-01-18 01:31:25,984 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 2
2025-01-18 01:31:25,984 - src.functions - DEBUG - Found rects: [Rect(252.51565551757812, 97.5888671875, 522.7468872070312, 108.6630859375), Rect(75.328125, 110.0888671875, 527.3569946289062, 121.1630859375), Rect(75.328125, 122.5888671875, 459.814208984375, 133.6630859375)]
2025-01-18 01:31:25,994 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 2
2025-01-18 01:31:25,994 - src.functions - DEBUG - Found rects: [Rect(75.328125, 522.0888671875, 517.880615234375, 533.1630859375), Rect(75.328125, 535.5888671875, 105.3177261352539, 546.6630859375)]
2025-01-18 01:31:26,002 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2
2025-01-18 01:31:26,003 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,003 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2.
2025-01-18 01:31:26,009 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-18 01:31:26,009 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,009 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-18 01:31:26,016 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2
2025-01-18 01:31:26,016 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,016 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2.
2025-01-18 01:31:26,023 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2
2025-01-18 01:31:26,023 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,023 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2.
2025-01-18 01:31:26,030 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 2
2025-01-18 01:31:26,030 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,030 - src.functions - WARNING - Could not find sentence 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 2.
2025-01-18 01:31:26,037 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 2
2025-01-18 01:31:26,037 - src.functions - DEBUG - Found rects: [Rect(369.5124206542969, 85.0888671875, 530.3182373046875, 96.1630859375), Rect(75.328125, 97.5888671875, 113.08060455322266, 108.6630859375)]
2025-01-18 01:31:26,045 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2
2025-01-18 01:31:26,045 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,045 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2.
2025-01-18 01:31:26,054 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-18 01:31:26,054 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,054 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-18 01:31:26,062 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3
2025-01-18 01:31:26,062 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,062 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3.
2025-01-18 01:31:26,071 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3
2025-01-18 01:31:26,071 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,071 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3.
2025-01-18 01:31:26,079 - src.functions - DEBUG - Searching for sentence #11: 'In contrast, the solid
line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the
two classes but also stays as far away from the closest training instances as possible' on page 3
2025-01-18 01:31:26,079 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,079 - src.functions - WARNING - Could not find sentence 'In contrast, the solid
line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the
two classes but also stays as far away from the closest training instances as possible' on page 3.
2025-01-18 01:31:26,088 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-18 01:31:26,088 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,088 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-18 01:31:26,098 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 3
2025-01-18 01:31:26,098 - src.functions - DEBUG - Found rects: [Rect(75.328125, 292.1767883300781, 140.65020751953125, 308.00177001953125), Rect(144.58544921875, 292.1767883300781, 175.27883911132812, 308.00177001953125), Rect(179.214111328125, 292.1767883300781, 271.3088073730469, 308.00177001953125), Rect(75.328125, 314.5888671875, 534.448974609375, 325.6630859375), Rect(75.328125, 327.0888671875, 170.83660888671875, 338.1630859375)]
2025-01-18 01:31:26,111 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3
2025-01-18 01:31:26,111 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,111 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3.
2025-01-18 01:31:26,121 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3
2025-01-18 01:31:26,121 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,121 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3.
2025-01-18 01:31:26,130 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3
2025-01-18 01:31:26,130 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,130 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3.
2025-01-18 01:31:26,139 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 3
2025-01-18 01:31:26,139 - src.functions - DEBUG - Found rects: [Rect(75.328125, 525.5888671875, 521.5821533203125, 536.6630859375), Rect(75.328125, 539.0888671875, 271.3769836425781, 550.1630859375)]
2025-01-18 01:31:26,149 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-18 01:31:26,149 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,150 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-18 01:31:26,158 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3
2025-01-18 01:31:26,158 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,158 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3.
2025-01-18 01:31:26,167 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3
2025-01-18 01:31:26,167 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,167 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3.
2025-01-18 01:31:26,175 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 3
2025-01-18 01:31:26,176 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,176 - src.functions - WARNING - Could not find sentence 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 3.
2025-01-18 01:31:26,184 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 3
2025-01-18 01:31:26,184 - src.functions - DEBUG - Found rects: []
2025-01-18 01:31:26,184 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 3.
2025-01-18 01:31:26,193 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 3
2025-01-18 01:31:26,193 - src.functions - DEBUG - Found rects: [Rect(175.83595275878906, 327.0888671875, 529.6141357421875, 338.1630859375), Rect(75.328125, 339.5888671875, 444.8943786621094, 350.6630859375)]
2025-01-18 14:30:52,782 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-18 14:30:52,783 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-18 14:30:52,783 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-18 14:30:52,783 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-18 14:30:52,784 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-18 14:30:52,784 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-18 14:30:52,837 - src.functions - DEBUG - Sentences: ['Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', ', hundreds to thousands of instances), especially for classification tasks', 'However, they\ndon’t scale very well to very large datasets, as you will see', 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 'The left plot shows the decision boundaries of three possible linear classifiers', 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 'First, it only works if the data is linearly\nseparable', 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', ', instances that end up in the middle of\nthe street or even on the wrong side)', 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 'With a high value, you get the model on the right', 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 'Then, as usual, you can use the model to make predictions:', 'The first plant is classified as an Iris virginica, while the second is not', 'Let’s look at the scores that the SVM used\nto make these predictions', 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 'This dataset is not linearly separable, as\nyou can see', 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 'You can generate this dataset using the \n function:\n1\n2\n1\n2']
2025-01-18 14:30:52,838 - src.functions - DEBUG - Numbered Sentences: {1: 'Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', 2: ', hundreds to thousands of instances), especially for classification tasks', 3: 'However, they\ndon’t scale very well to very large datasets, as you will see', 4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 5: 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 6: 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 7: 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 8: 'The left plot shows the decision boundaries of three possible linear classifiers', 9: 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 10: 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 11: 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 12: 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 13: 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 14: 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 15: 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 16: 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', 17: ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 19: 'First, it only works if the data is linearly\nseparable', 20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 21: 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 22: 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', 23: ', instances that end up in the middle of\nthe street or even on the wrong side)', 24: 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 25: 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 26: 'With a high value, you get the model on the right', 27: 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 28: 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 29: 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 30: 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 32: 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 33: 'Then, as usual, you can use the model to make predictions:', 34: 'The first plant is classified as an Iris virginica, while the second is not', 35: 'Let’s look at the scores that the SVM used\nto make these predictions', 36: 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 37: 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 38: 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 39: 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 41: 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 42: 'This dataset is not linearly separable, as\nyou can see', 43: 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 44: 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 46: 'You can generate this dataset using the \n function:\n1\n2\n1\n2'}
2025-01-18 14:30:53,239 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-18 14:30:54,916 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-18 14:30:54,916 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-18 14:30:54,916 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-18 14:30:54,916 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-18 14:30:54,916 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-18 14:30:54,917 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-18 14:30:54,958 - src.functions - DEBUG - Sentences: ['Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', ', hundreds to thousands of instances), especially for classification tasks', 'However, they\ndon’t scale very well to very large datasets, as you will see', 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 'The left plot shows the decision boundaries of three possible linear classifiers', 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 'First, it only works if the data is linearly\nseparable', 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', ', instances that end up in the middle of\nthe street or even on the wrong side)', 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 'With a high value, you get the model on the right', 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 'Then, as usual, you can use the model to make predictions:', 'The first plant is classified as an Iris virginica, while the second is not', 'Let’s look at the scores that the SVM used\nto make these predictions', 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 'This dataset is not linearly separable, as\nyou can see', 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 'You can generate this dataset using the \n function:\n1\n2\n1\n2']
2025-01-18 14:30:54,959 - src.functions - DEBUG - Numbered Sentences: {1: 'Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', 2: ', hundreds to thousands of instances), especially for classification tasks', 3: 'However, they\ndon’t scale very well to very large datasets, as you will see', 4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 5: 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 6: 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 7: 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 8: 'The left plot shows the decision boundaries of three possible linear classifiers', 9: 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 10: 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 11: 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 12: 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 13: 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 14: 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 15: 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 16: 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', 17: ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 19: 'First, it only works if the data is linearly\nseparable', 20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 21: 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 22: 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', 23: ', instances that end up in the middle of\nthe street or even on the wrong side)', 24: 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 25: 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 26: 'With a high value, you get the model on the right', 27: 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 28: 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 29: 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 30: 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 32: 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 33: 'Then, as usual, you can use the model to make predictions:', 34: 'The first plant is classified as an Iris virginica, while the second is not', 35: 'Let’s look at the scores that the SVM used\nto make these predictions', 36: 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 37: 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 38: 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 39: 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 41: 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 42: 'This dataset is not linearly separable, as\nyou can see', 43: 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 44: 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 46: 'You can generate this dataset using the \n function:\n1\n2\n1\n2'}
2025-01-18 14:30:54,962 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-18 14:30:57,013 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-18 14:30:57,015 - src.functions - DEBUG - ChatGPT Output: ```json
{
    "Key Concepts": [
        1,
        4,
        5,
        18,
        39,
       40
    ],
    "Examples": [
        6,
        20,
        31,
        45
    ],
    "Definitions": [
        1,
        12,
        14,
        15,
        19,
        22,
        28,
        39,
        41
    ]
}
```
2025-01-18 14:30:57,023 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-18 14:30:57,023 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-18 14:30:57,049 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 1
2025-01-18 14:30:57,049 - src.functions - DEBUG - Found rects: [Rect(75.328125, 182.0888671875, 438.5721130371094, 193.1630859375)]
2025-01-18 14:30:57,057 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 1
2025-01-18 14:30:57,057 - src.functions - DEBUG - Found rects: [Rect(443.5721130371094, 182.0888671875, 521.6256103515625, 193.1630859375), Rect(75.328125, 214.1768035888672, 117.83416748046875, 230.00177001953125), Rect(121.76940155029297, 214.1768035888672, 152.46279907226562, 230.00177001953125), Rect(156.39804077148438, 214.1768035888672, 248.49273681640625, 230.00177001953125), Rect(75.328125, 236.5888671875, 363.6017150878906, 247.6630859375)]
2025-01-18 14:30:57,071 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-18 14:30:57,071 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,071 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-18 14:30:57,074 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1
2025-01-18 14:30:57,074 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,074 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1.
2025-01-18 14:30:57,077 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1
2025-01-18 14:30:57,077 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,077 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1.
2025-01-18 14:30:57,081 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 1
2025-01-18 14:30:57,081 - src.functions - DEBUG - Found rects: [Rect(368.6094055175781, 236.5888671875, 527.1851806640625, 247.6630859375), Rect(75.328125, 249.0888671875, 247.49119567871094, 260.1630859375)]
2025-01-18 14:30:57,087 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1
2025-01-18 14:30:57,087 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,087 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1.
2025-01-18 14:30:57,089 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1
2025-01-18 14:30:57,089 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,089 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1.
2025-01-18 14:30:57,092 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1
2025-01-18 14:30:57,092 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,092 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1.
2025-01-18 14:30:57,094 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-18 14:30:57,094 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-18 14:30:57,103 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 1
2025-01-18 14:30:57,103 - src.functions - DEBUG - Found rects: [Rect(415.2421875, 324.0888671875, 517.8462524414062, 335.1630859375), Rect(75.328125, 336.5888671875, 499.3355712890625, 347.6630859375)]
2025-01-18 14:30:57,108 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 1
2025-01-18 14:30:57,108 - src.functions - DEBUG - Found rects: [Rect(389.6484375, 468.5888671875, 508.47076416015625, 479.6630859375), Rect(75.328125, 481.0888671875, 262.767578125, 492.1630859375)]
2025-01-18 14:30:57,113 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 1
2025-01-18 14:30:57,113 - src.functions - DEBUG - Found rects: [Rect(280.1015625, 515.4517822265625, 331.3959655761719, 526.6181030273438), Rect(94.828125, 532.816650390625, 302.26629638671875, 541.122314453125)]
2025-01-18 14:30:57,117 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 1
2025-01-18 14:30:57,117 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,117 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 1.
2025-01-18 14:30:57,119 - src.functions - DEBUG - Searching for sentence #22: 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 1
2025-01-18 14:30:57,119 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,119 - src.functions - WARNING - Could not find sentence 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 1.
2025-01-18 14:30:57,121 - src.functions - DEBUG - Searching for sentence #28: 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 1
2025-01-18 14:30:57,121 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,121 - src.functions - WARNING - Could not find sentence 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 1.
2025-01-18 14:30:57,123 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1
2025-01-18 14:30:57,123 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,123 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1.
2025-01-18 14:30:57,125 - src.functions - DEBUG - Searching for sentence #41: 'Consider the lefthand
plot in Figure 5-5: it represents a simple dataset with just one feature, x' on page 1
2025-01-18 14:30:57,125 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,125 - src.functions - WARNING - Could not find sentence 'Consider the lefthand
plot in Figure 5-5: it represents a simple dataset with just one feature, x' on page 1.
2025-01-18 14:30:57,131 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-18 14:30:57,131 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,131 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-18 14:30:57,138 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2
2025-01-18 14:30:57,138 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,138 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2.
2025-01-18 14:30:57,145 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2
2025-01-18 14:30:57,145 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,145 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2.
2025-01-18 14:30:57,152 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-18 14:30:57,152 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-18 14:30:57,162 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2
2025-01-18 14:30:57,162 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,162 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2.
2025-01-18 14:30:57,169 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2
2025-01-18 14:30:57,169 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,169 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2.
2025-01-18 14:30:57,176 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2
2025-01-18 14:30:57,176 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,176 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2.
2025-01-18 14:30:57,183 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 2
2025-01-18 14:30:57,183 - src.functions - DEBUG - Found rects: [Rect(252.51565551757812, 97.5888671875, 522.7468872070312, 108.6630859375), Rect(75.328125, 110.0888671875, 527.3569946289062, 121.1630859375), Rect(75.328125, 122.5888671875, 459.814208984375, 133.6630859375)]
2025-01-18 14:30:57,193 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 2
2025-01-18 14:30:57,193 - src.functions - DEBUG - Found rects: [Rect(75.328125, 522.0888671875, 517.880615234375, 533.1630859375), Rect(75.328125, 535.5888671875, 105.3177261352539, 546.6630859375)]
2025-01-18 14:30:57,202 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2
2025-01-18 14:30:57,202 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,202 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2.
2025-01-18 14:30:57,209 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-18 14:30:57,209 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,209 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-18 14:30:57,215 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2
2025-01-18 14:30:57,215 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,215 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2.
2025-01-18 14:30:57,222 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2
2025-01-18 14:30:57,222 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,222 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2.
2025-01-18 14:30:57,229 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 2
2025-01-18 14:30:57,229 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,229 - src.functions - WARNING - Could not find sentence 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 2.
2025-01-18 14:30:57,236 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 2
2025-01-18 14:30:57,236 - src.functions - DEBUG - Found rects: [Rect(369.5124206542969, 85.0888671875, 530.3182373046875, 96.1630859375), Rect(75.328125, 97.5888671875, 113.08060455322266, 108.6630859375)]
2025-01-18 14:30:57,245 - src.functions - DEBUG - Searching for sentence #22: 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 2
2025-01-18 14:30:57,245 - src.functions - DEBUG - Found rects: [Rect(319.5546875, 234.0888671875, 510.58648681640625, 245.1630859375), Rect(75.328125, 246.5888671875, 372.4911193847656, 257.6630859375)]
2025-01-18 14:30:57,254 - src.functions - DEBUG - Searching for sentence #28: 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 2
2025-01-18 14:30:57,254 - src.functions - DEBUG - Found rects: [Rect(248.72975158691406, 317.0888671875, 349.5337219238281, 328.1630859375), Rect(354.5234375, 317.0888671875, 515.6038208007812, 328.1630859375), Rect(75.328125, 329.5888671875, 230.57064819335938, 340.6630859375)]
2025-01-18 14:30:57,264 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2
2025-01-18 14:30:57,264 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,264 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2.
2025-01-18 14:30:57,270 - src.functions - DEBUG - Searching for sentence #41: 'Consider the lefthand
plot in Figure 5-5: it represents a simple dataset with just one feature, x' on page 2
2025-01-18 14:30:57,270 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,270 - src.functions - WARNING - Could not find sentence 'Consider the lefthand
plot in Figure 5-5: it represents a simple dataset with just one feature, x' on page 2.
2025-01-18 14:30:57,279 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-18 14:30:57,279 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,279 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-18 14:30:57,287 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3
2025-01-18 14:30:57,287 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,287 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3.
2025-01-18 14:30:57,296 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3
2025-01-18 14:30:57,296 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,296 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3.
2025-01-18 14:30:57,304 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-18 14:30:57,304 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,304 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-18 14:30:57,313 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 3
2025-01-18 14:30:57,313 - src.functions - DEBUG - Found rects: [Rect(75.328125, 292.1767883300781, 140.65020751953125, 308.00177001953125), Rect(144.58544921875, 292.1767883300781, 175.27883911132812, 308.00177001953125), Rect(179.214111328125, 292.1767883300781, 271.3088073730469, 308.00177001953125), Rect(75.328125, 314.5888671875, 534.448974609375, 325.6630859375), Rect(75.328125, 327.0888671875, 170.83660888671875, 338.1630859375)]
2025-01-18 14:30:57,327 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 3
2025-01-18 14:30:57,327 - src.functions - DEBUG - Found rects: [Rect(175.83595275878906, 327.0888671875, 529.6141357421875, 338.1630859375), Rect(75.328125, 339.5888671875, 444.8943786621094, 350.6630859375)]
2025-01-18 14:30:57,337 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3
2025-01-18 14:30:57,337 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,337 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3.
2025-01-18 14:30:57,346 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3
2025-01-18 14:30:57,346 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,346 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3.
2025-01-18 14:30:57,357 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3
2025-01-18 14:30:57,357 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,357 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3.
2025-01-18 14:30:57,368 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 3
2025-01-18 14:30:57,368 - src.functions - DEBUG - Found rects: [Rect(75.328125, 525.5888671875, 521.5821533203125, 536.6630859375), Rect(75.328125, 539.0888671875, 271.3769836425781, 550.1630859375)]
2025-01-18 14:30:57,378 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-18 14:30:57,378 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,378 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-18 14:30:57,387 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3
2025-01-18 14:30:57,387 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,387 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3.
2025-01-18 14:30:57,395 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3
2025-01-18 14:30:57,395 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,395 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3.
2025-01-18 14:30:57,404 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 3
2025-01-18 14:30:57,404 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,404 - src.functions - WARNING - Could not find sentence 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 3.
2025-01-18 14:30:57,412 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 3
2025-01-18 14:30:57,412 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,412 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 3.
2025-01-18 14:30:57,421 - src.functions - DEBUG - Searching for sentence #22: 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 3
2025-01-18 14:30:57,421 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,421 - src.functions - WARNING - Could not find sentence 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 3.
2025-01-18 14:30:57,429 - src.functions - DEBUG - Searching for sentence #28: 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 3
2025-01-18 14:30:57,429 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:57,429 - src.functions - WARNING - Could not find sentence 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 3.
2025-01-18 14:30:57,438 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 3
2025-01-18 14:30:57,438 - src.functions - DEBUG - Found rects: [Rect(75.328125, 292.1767883300781, 140.65020751953125, 308.00177001953125), Rect(144.58544921875, 292.1767883300781, 175.27883911132812, 308.00177001953125), Rect(179.214111328125, 292.1767883300781, 271.3088073730469, 308.00177001953125), Rect(75.328125, 314.5888671875, 534.448974609375, 325.6630859375), Rect(75.328125, 327.0888671875, 170.83660888671875, 338.1630859375)]
2025-01-18 14:30:57,452 - src.functions - DEBUG - Searching for sentence #41: 'Consider the lefthand
plot in Figure 5-5: it represents a simple dataset with just one feature, x' on page 3
2025-01-18 14:30:57,452 - src.functions - DEBUG - Found rects: [Rect(449.89404296875, 339.5888671875, 535.97998046875, 350.6630859375), Rect(75.328125, 352.0888671875, 359.6884765625, 363.1630859375)]
2025-01-18 14:30:58,464 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-18 14:30:58,470 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    11,
    18,
    39
  ],
  "Examples": [
    6,
    20,
    31,
    34,
    45
  ],
  "Definitions": [
    1,
    12,
    14,
    15,
    19,
    41,
    44
  ]
}
```
2025-01-18 14:30:58,480 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-18 14:30:58,481 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-18 14:30:58,506 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 1
2025-01-18 14:30:58,506 - src.functions - DEBUG - Found rects: [Rect(75.328125, 182.0888671875, 438.5721130371094, 193.1630859375)]
2025-01-18 14:30:58,514 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 1
2025-01-18 14:30:58,514 - src.functions - DEBUG - Found rects: [Rect(443.5721130371094, 182.0888671875, 521.6256103515625, 193.1630859375), Rect(75.328125, 214.1768035888672, 117.83416748046875, 230.00177001953125), Rect(121.76940155029297, 214.1768035888672, 152.46279907226562, 230.00177001953125), Rect(156.39804077148438, 214.1768035888672, 248.49273681640625, 230.00177001953125), Rect(75.328125, 236.5888671875, 363.6017150878906, 247.6630859375)]
2025-01-18 14:30:58,528 - src.functions - DEBUG - Searching for sentence #11: 'In contrast, the solid
line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the
two classes but also stays as far away from the closest training instances as possible' on page 1
2025-01-18 14:30:58,528 - src.functions - DEBUG - Found rects: [Rect(444.39410400390625, 299.0888671875, 526.0419921875, 310.1630859375), Rect(75.328125, 311.5888671875, 527.990478515625, 322.6630859375), Rect(75.328125, 324.0888671875, 410.24267578125, 335.1630859375)]
2025-01-18 14:30:58,537 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-18 14:30:58,537 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,537 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-18 14:30:58,539 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1
2025-01-18 14:30:58,539 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,540 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1.
2025-01-18 14:30:58,543 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 1
2025-01-18 14:30:58,543 - src.functions - DEBUG - Found rects: [Rect(368.6094055175781, 236.5888671875, 527.1851806640625, 247.6630859375), Rect(75.328125, 249.0888671875, 247.49119567871094, 260.1630859375)]
2025-01-18 14:30:58,548 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1
2025-01-18 14:30:58,548 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,548 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1.
2025-01-18 14:30:58,551 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1
2025-01-18 14:30:58,551 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,551 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1.
2025-01-18 14:30:58,553 - src.functions - DEBUG - Searching for sentence #34: 'The first plant is classified as an Iris virginica, while the second is not' on page 1
2025-01-18 14:30:58,553 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,553 - src.functions - WARNING - Could not find sentence 'The first plant is classified as an Iris virginica, while the second is not' on page 1.
2025-01-18 14:30:58,555 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1
2025-01-18 14:30:58,555 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,556 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1.
2025-01-18 14:30:58,558 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-18 14:30:58,558 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-18 14:30:58,566 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 1
2025-01-18 14:30:58,566 - src.functions - DEBUG - Found rects: [Rect(415.2421875, 324.0888671875, 517.8462524414062, 335.1630859375), Rect(75.328125, 336.5888671875, 499.3355712890625, 347.6630859375)]
2025-01-18 14:30:58,571 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 1
2025-01-18 14:30:58,571 - src.functions - DEBUG - Found rects: [Rect(389.6484375, 468.5888671875, 508.47076416015625, 479.6630859375), Rect(75.328125, 481.0888671875, 262.767578125, 492.1630859375)]
2025-01-18 14:30:58,576 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 1
2025-01-18 14:30:58,576 - src.functions - DEBUG - Found rects: [Rect(280.1015625, 515.4517822265625, 331.3959655761719, 526.6181030273438), Rect(94.828125, 532.816650390625, 302.26629638671875, 541.122314453125)]
2025-01-18 14:30:58,580 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 1
2025-01-18 14:30:58,580 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,580 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 1.
2025-01-18 14:30:58,582 - src.functions - DEBUG - Searching for sentence #41: 'Consider the lefthand
plot in Figure 5-5: it represents a simple dataset with just one feature, x' on page 1
2025-01-18 14:30:58,582 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,582 - src.functions - WARNING - Could not find sentence 'Consider the lefthand
plot in Figure 5-5: it represents a simple dataset with just one feature, x' on page 1.
2025-01-18 14:30:58,583 - src.functions - DEBUG - Searching for sentence #44: 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 1
2025-01-18 14:30:58,583 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,584 - src.functions - WARNING - Could not find sentence 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 1.
2025-01-18 14:30:58,590 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-18 14:30:58,590 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,590 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-18 14:30:58,597 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2
2025-01-18 14:30:58,597 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,597 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2.
2025-01-18 14:30:58,604 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2
2025-01-18 14:30:58,604 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,604 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2.
2025-01-18 14:30:58,610 - src.functions - DEBUG - Searching for sentence #11: 'In contrast, the solid
line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the
two classes but also stays as far away from the closest training instances as possible' on page 2
2025-01-18 14:30:58,611 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,611 - src.functions - WARNING - Could not find sentence 'In contrast, the solid
line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the
two classes but also stays as far away from the closest training instances as possible' on page 2.
2025-01-18 14:30:58,617 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-18 14:30:58,617 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-18 14:30:58,626 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2
2025-01-18 14:30:58,626 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,626 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2.
2025-01-18 14:30:58,633 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2
2025-01-18 14:30:58,633 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,633 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2.
2025-01-18 14:30:58,640 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 2
2025-01-18 14:30:58,640 - src.functions - DEBUG - Found rects: [Rect(252.51565551757812, 97.5888671875, 522.7468872070312, 108.6630859375), Rect(75.328125, 110.0888671875, 527.3569946289062, 121.1630859375), Rect(75.328125, 122.5888671875, 459.814208984375, 133.6630859375)]
2025-01-18 14:30:58,650 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 2
2025-01-18 14:30:58,650 - src.functions - DEBUG - Found rects: [Rect(75.328125, 522.0888671875, 517.880615234375, 533.1630859375), Rect(75.328125, 535.5888671875, 105.3177261352539, 546.6630859375)]
2025-01-18 14:30:58,659 - src.functions - DEBUG - Searching for sentence #34: 'The first plant is classified as an Iris virginica, while the second is not' on page 2
2025-01-18 14:30:58,659 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,659 - src.functions - WARNING - Could not find sentence 'The first plant is classified as an Iris virginica, while the second is not' on page 2.
2025-01-18 14:30:58,665 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2
2025-01-18 14:30:58,665 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,665 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2.
2025-01-18 14:30:58,672 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-18 14:30:58,672 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,672 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-18 14:30:58,679 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2
2025-01-18 14:30:58,679 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,679 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2.
2025-01-18 14:30:58,686 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2
2025-01-18 14:30:58,686 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,686 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2.
2025-01-18 14:30:58,692 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 2
2025-01-18 14:30:58,692 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,692 - src.functions - WARNING - Could not find sentence 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 2.
2025-01-18 14:30:58,699 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 2
2025-01-18 14:30:58,699 - src.functions - DEBUG - Found rects: [Rect(369.5124206542969, 85.0888671875, 530.3182373046875, 96.1630859375), Rect(75.328125, 97.5888671875, 113.08060455322266, 108.6630859375)]
2025-01-18 14:30:58,708 - src.functions - DEBUG - Searching for sentence #41: 'Consider the lefthand
plot in Figure 5-5: it represents a simple dataset with just one feature, x' on page 2
2025-01-18 14:30:58,708 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,708 - src.functions - WARNING - Could not find sentence 'Consider the lefthand
plot in Figure 5-5: it represents a simple dataset with just one feature, x' on page 2.
2025-01-18 14:30:58,715 - src.functions - DEBUG - Searching for sentence #44: 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 2
2025-01-18 14:30:58,715 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,715 - src.functions - WARNING - Could not find sentence 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 2.
2025-01-18 14:30:58,723 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-18 14:30:58,723 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,723 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-18 14:30:58,732 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3
2025-01-18 14:30:58,732 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,732 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3.
2025-01-18 14:30:58,740 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3
2025-01-18 14:30:58,740 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,740 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3.
2025-01-18 14:30:58,749 - src.functions - DEBUG - Searching for sentence #11: 'In contrast, the solid
line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the
two classes but also stays as far away from the closest training instances as possible' on page 3
2025-01-18 14:30:58,749 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,749 - src.functions - WARNING - Could not find sentence 'In contrast, the solid
line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the
two classes but also stays as far away from the closest training instances as possible' on page 3.
2025-01-18 14:30:58,757 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-18 14:30:58,757 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,757 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-18 14:30:58,766 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 3
2025-01-18 14:30:58,766 - src.functions - DEBUG - Found rects: [Rect(75.328125, 292.1767883300781, 140.65020751953125, 308.00177001953125), Rect(144.58544921875, 292.1767883300781, 175.27883911132812, 308.00177001953125), Rect(179.214111328125, 292.1767883300781, 271.3088073730469, 308.00177001953125), Rect(75.328125, 314.5888671875, 534.448974609375, 325.6630859375), Rect(75.328125, 327.0888671875, 170.83660888671875, 338.1630859375)]
2025-01-18 14:30:58,779 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3
2025-01-18 14:30:58,779 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,779 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3.
2025-01-18 14:30:58,788 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3
2025-01-18 14:30:58,788 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,788 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3.
2025-01-18 14:30:58,796 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3
2025-01-18 14:30:58,796 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,796 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3.
2025-01-18 14:30:58,805 - src.functions - DEBUG - Searching for sentence #34: 'The first plant is classified as an Iris virginica, while the second is not' on page 3
2025-01-18 14:30:58,805 - src.functions - DEBUG - Found rects: [Rect(75.328125, 112.0888671875, 355.44525146484375, 123.1630859375)]
2025-01-18 14:30:58,815 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 3
2025-01-18 14:30:58,815 - src.functions - DEBUG - Found rects: [Rect(75.328125, 525.5888671875, 521.5821533203125, 536.6630859375), Rect(75.328125, 539.0888671875, 271.3769836425781, 550.1630859375)]
2025-01-18 14:30:58,825 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-18 14:30:58,825 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,825 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-18 14:30:58,834 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3
2025-01-18 14:30:58,834 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,834 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3.
2025-01-18 14:30:58,842 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3
2025-01-18 14:30:58,842 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,842 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3.
2025-01-18 14:30:58,851 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 3
2025-01-18 14:30:58,851 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,851 - src.functions - WARNING - Could not find sentence 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 3.
2025-01-18 14:30:58,860 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 3
2025-01-18 14:30:58,860 - src.functions - DEBUG - Found rects: []
2025-01-18 14:30:58,860 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 3.
2025-01-18 14:30:58,869 - src.functions - DEBUG - Searching for sentence #41: 'Consider the lefthand
plot in Figure 5-5: it represents a simple dataset with just one feature, x' on page 3
2025-01-18 14:30:58,869 - src.functions - DEBUG - Found rects: [Rect(449.89404296875, 339.5888671875, 535.97998046875, 350.6630859375), Rect(75.328125, 352.0888671875, 359.6884765625, 363.1630859375)]
2025-01-18 14:30:58,880 - src.functions - DEBUG - Searching for sentence #44: 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 3
2025-01-18 14:30:58,880 - src.functions - DEBUG - Found rects: [Rect(244.54690551757812, 481.816650390625, 403.197998046875, 490.122314453125), Rect(75.328125, 499.5888671875, 402.28619384765625, 510.6630859375), Rect(75.328125, 513.0888671875, 346.64215087890625, 524.1630859375), Rect(416.6328125, 513.0888671875, 443.0087890625, 524.1630859375), Rect(488.0, 513.0888671875, 526.592041015625, 524.1630859375)]
2025-01-18 15:50:35,709 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-18 15:50:35,709 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-18 15:50:35,710 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-18 15:50:35,710 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-18 15:50:35,710 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-18 15:50:35,710 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-18 15:50:35,759 - src.functions - DEBUG - Sentences: ['Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', ', hundreds to thousands of instances), especially for classification tasks', 'However, they\ndon’t scale very well to very large datasets, as you will see', 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 'The left plot shows the decision boundaries of three possible linear classifiers', 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 'First, it only works if the data is linearly\nseparable', 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', ', instances that end up in the middle of\nthe street or even on the wrong side)', 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 'With a high value, you get the model on the right', 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 'Then, as usual, you can use the model to make predictions:', 'The first plant is classified as an Iris virginica, while the second is not', 'Let’s look at the scores that the SVM used\nto make these predictions', 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 'This dataset is not linearly separable, as\nyou can see', 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 'You can generate this dataset using the \n function:\n1\n2\n1\n2']
2025-01-18 15:50:35,759 - src.functions - DEBUG - Numbered Sentences: {1: 'Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', 2: ', hundreds to thousands of instances), especially for classification tasks', 3: 'However, they\ndon’t scale very well to very large datasets, as you will see', 4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 5: 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 6: 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 7: 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 8: 'The left plot shows the decision boundaries of three possible linear classifiers', 9: 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 10: 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 11: 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 12: 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 13: 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 14: 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 15: 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 16: 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', 17: ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 19: 'First, it only works if the data is linearly\nseparable', 20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 21: 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 22: 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', 23: ', instances that end up in the middle of\nthe street or even on the wrong side)', 24: 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 25: 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 26: 'With a high value, you get the model on the right', 27: 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 28: 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 29: 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 30: 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 32: 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 33: 'Then, as usual, you can use the model to make predictions:', 34: 'The first plant is classified as an Iris virginica, while the second is not', 35: 'Let’s look at the scores that the SVM used\nto make these predictions', 36: 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 37: 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 38: 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 39: 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 41: 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 42: 'This dataset is not linearly separable, as\nyou can see', 43: 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 44: 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 46: 'You can generate this dataset using the \n function:\n1\n2\n1\n2'}
2025-01-18 15:50:35,831 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-18 15:50:39,120 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-18 15:50:39,123 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    12,
    18,
    21,
    39
  ],
  "Examples": [
    6,
    20,
    31,
    45
  ],
  "Definitions": [
    1,
    14,
    18,
    22,
    28
  ]
}
```
2025-01-18 15:50:39,135 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-18 15:50:39,135 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-18 15:50:39,161 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 1
2025-01-18 15:50:39,161 - src.functions - DEBUG - Found rects: [Rect(75.328125, 182.0888671875, 438.5721130371094, 193.1630859375)]
2025-01-18 15:50:39,169 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 1
2025-01-18 15:50:39,169 - src.functions - DEBUG - Found rects: [Rect(443.5721130371094, 182.0888671875, 521.6256103515625, 193.1630859375), Rect(75.328125, 214.1768035888672, 117.83416748046875, 230.00177001953125), Rect(121.76940155029297, 214.1768035888672, 152.46279907226562, 230.00177001953125), Rect(156.39804077148438, 214.1768035888672, 248.49273681640625, 230.00177001953125), Rect(75.328125, 236.5888671875, 363.6017150878906, 247.6630859375)]
2025-01-18 15:50:39,183 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 1
2025-01-18 15:50:39,183 - src.functions - DEBUG - Found rects: [Rect(415.2421875, 324.0888671875, 517.8462524414062, 335.1630859375), Rect(75.328125, 336.5888671875, 499.3355712890625, 347.6630859375)]
2025-01-18 15:50:39,189 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-18 15:50:39,190 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,190 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-18 15:50:39,193 - src.functions - DEBUG - Searching for sentence #21: 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 1
2025-01-18 15:50:39,193 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,193 - src.functions - WARNING - Could not find sentence 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 1.
2025-01-18 15:50:39,195 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1
2025-01-18 15:50:39,195 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,195 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1.
2025-01-18 15:50:39,198 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 1
2025-01-18 15:50:39,198 - src.functions - DEBUG - Found rects: [Rect(368.6094055175781, 236.5888671875, 527.1851806640625, 247.6630859375), Rect(75.328125, 249.0888671875, 247.49119567871094, 260.1630859375)]
2025-01-18 15:50:39,203 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1
2025-01-18 15:50:39,203 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,203 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1.
2025-01-18 15:50:39,206 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1
2025-01-18 15:50:39,206 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,206 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1.
2025-01-18 15:50:39,208 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1
2025-01-18 15:50:39,208 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,208 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1.
2025-01-18 15:50:39,211 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-18 15:50:39,211 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-18 15:50:39,219 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 1
2025-01-18 15:50:39,219 - src.functions - DEBUG - Found rects: [Rect(389.6484375, 468.5888671875, 508.47076416015625, 479.6630859375), Rect(75.328125, 481.0888671875, 262.767578125, 492.1630859375)]
2025-01-18 15:50:39,223 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-18 15:50:39,223 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,223 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-18 15:50:39,225 - src.functions - DEBUG - Searching for sentence #22: 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 1
2025-01-18 15:50:39,225 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,225 - src.functions - WARNING - Could not find sentence 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 1.
2025-01-18 15:50:39,227 - src.functions - DEBUG - Searching for sentence #28: 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 1
2025-01-18 15:50:39,227 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,227 - src.functions - WARNING - Could not find sentence 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 1.
2025-01-18 15:50:39,234 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-18 15:50:39,234 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,234 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-18 15:50:39,240 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2
2025-01-18 15:50:39,240 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,240 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2.
2025-01-18 15:50:39,247 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2
2025-01-18 15:50:39,247 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,247 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2.
2025-01-18 15:50:39,254 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2
2025-01-18 15:50:39,254 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,254 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2.
2025-01-18 15:50:39,261 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-18 15:50:39,261 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-18 15:50:39,270 - src.functions - DEBUG - Searching for sentence #21: 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 2
2025-01-18 15:50:39,270 - src.functions - DEBUG - Found rects: [Rect(272.28125, 217.316650390625, 375.4692077636719, 225.622314453125), Rect(75.328125, 234.0888671875, 314.55462646484375, 245.1630859375)]
2025-01-18 15:50:39,279 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2
2025-01-18 15:50:39,279 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,279 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2.
2025-01-18 15:50:39,285 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2
2025-01-18 15:50:39,285 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,285 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2.
2025-01-18 15:50:39,292 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 2
2025-01-18 15:50:39,293 - src.functions - DEBUG - Found rects: [Rect(252.51565551757812, 97.5888671875, 522.7468872070312, 108.6630859375), Rect(75.328125, 110.0888671875, 527.3569946289062, 121.1630859375), Rect(75.328125, 122.5888671875, 459.814208984375, 133.6630859375)]
2025-01-18 15:50:39,303 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 2
2025-01-18 15:50:39,303 - src.functions - DEBUG - Found rects: [Rect(75.328125, 522.0888671875, 517.880615234375, 533.1630859375), Rect(75.328125, 535.5888671875, 105.3177261352539, 546.6630859375)]
2025-01-18 15:50:39,311 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2
2025-01-18 15:50:39,311 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,312 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2.
2025-01-18 15:50:39,318 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-18 15:50:39,318 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,318 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-18 15:50:39,325 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2
2025-01-18 15:50:39,325 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,325 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2.
2025-01-18 15:50:39,332 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-18 15:50:39,332 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-18 15:50:39,341 - src.functions - DEBUG - Searching for sentence #22: 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 2
2025-01-18 15:50:39,341 - src.functions - DEBUG - Found rects: [Rect(319.5546875, 234.0888671875, 510.58648681640625, 245.1630859375), Rect(75.328125, 246.5888671875, 372.4911193847656, 257.6630859375)]
2025-01-18 15:50:39,350 - src.functions - DEBUG - Searching for sentence #28: 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 2
2025-01-18 15:50:39,350 - src.functions - DEBUG - Found rects: [Rect(248.72975158691406, 317.0888671875, 349.5337219238281, 328.1630859375), Rect(354.5234375, 317.0888671875, 515.6038208007812, 328.1630859375), Rect(75.328125, 329.5888671875, 230.57064819335938, 340.6630859375)]
2025-01-18 15:50:39,362 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-18 15:50:39,362 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,362 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-18 15:50:39,370 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3
2025-01-18 15:50:39,370 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,370 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3.
2025-01-18 15:50:39,379 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3
2025-01-18 15:50:39,379 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,379 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3.
2025-01-18 15:50:39,387 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3
2025-01-18 15:50:39,387 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,387 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3.
2025-01-18 15:50:39,396 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-18 15:50:39,396 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,396 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-18 15:50:39,404 - src.functions - DEBUG - Searching for sentence #21: 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 3
2025-01-18 15:50:39,404 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,404 - src.functions - WARNING - Could not find sentence 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 3.
2025-01-18 15:50:39,413 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 3
2025-01-18 15:50:39,413 - src.functions - DEBUG - Found rects: [Rect(75.328125, 292.1767883300781, 140.65020751953125, 308.00177001953125), Rect(144.58544921875, 292.1767883300781, 175.27883911132812, 308.00177001953125), Rect(179.214111328125, 292.1767883300781, 271.3088073730469, 308.00177001953125), Rect(75.328125, 314.5888671875, 534.448974609375, 325.6630859375), Rect(75.328125, 327.0888671875, 170.83660888671875, 338.1630859375)]
2025-01-18 15:50:39,426 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3
2025-01-18 15:50:39,426 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,426 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3.
2025-01-18 15:50:39,434 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3
2025-01-18 15:50:39,434 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,435 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3.
2025-01-18 15:50:39,443 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3
2025-01-18 15:50:39,443 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,443 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3.
2025-01-18 15:50:39,452 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 3
2025-01-18 15:50:39,452 - src.functions - DEBUG - Found rects: [Rect(75.328125, 525.5888671875, 521.5821533203125, 536.6630859375), Rect(75.328125, 539.0888671875, 271.3769836425781, 550.1630859375)]
2025-01-18 15:50:39,463 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-18 15:50:39,463 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,463 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-18 15:50:39,471 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3
2025-01-18 15:50:39,471 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,471 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3.
2025-01-18 15:50:39,480 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-18 15:50:39,480 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,480 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-18 15:50:39,488 - src.functions - DEBUG - Searching for sentence #22: 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 3
2025-01-18 15:50:39,488 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,488 - src.functions - WARNING - Could not find sentence 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 3.
2025-01-18 15:50:39,497 - src.functions - DEBUG - Searching for sentence #28: 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 3
2025-01-18 15:50:39,497 - src.functions - DEBUG - Found rects: []
2025-01-18 15:50:39,497 - src.functions - WARNING - Could not find sentence 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 3.
2025-01-18 15:54:57,359 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-18 15:54:57,359 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-18 15:54:57,359 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-18 15:54:57,360 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-18 15:54:57,360 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-18 15:54:57,360 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-18 15:54:57,407 - src.functions - DEBUG - Sentences: ['Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', ', hundreds to thousands of instances), especially for classification tasks', 'However, they\ndon’t scale very well to very large datasets, as you will see', 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 'The left plot shows the decision boundaries of three possible linear classifiers', 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 'First, it only works if the data is linearly\nseparable', 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', ', instances that end up in the middle of\nthe street or even on the wrong side)', 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 'With a high value, you get the model on the right', 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 'Then, as usual, you can use the model to make predictions:', 'The first plant is classified as an Iris virginica, while the second is not', 'Let’s look at the scores that the SVM used\nto make these predictions', 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 'This dataset is not linearly separable, as\nyou can see', 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 'You can generate this dataset using the \n function:\n1\n2\n1\n2']
2025-01-18 15:54:57,407 - src.functions - DEBUG - Numbered Sentences: {1: 'Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', 2: ', hundreds to thousands of instances), especially for classification tasks', 3: 'However, they\ndon’t scale very well to very large datasets, as you will see', 4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 5: 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 6: 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 7: 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 8: 'The left plot shows the decision boundaries of three possible linear classifiers', 9: 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 10: 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 11: 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 12: 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 13: 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 14: 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 15: 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 16: 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', 17: ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 19: 'First, it only works if the data is linearly\nseparable', 20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 21: 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 22: 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', 23: ', instances that end up in the middle of\nthe street or even on the wrong side)', 24: 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 25: 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 26: 'With a high value, you get the model on the right', 27: 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 28: 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 29: 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 30: 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 32: 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 33: 'Then, as usual, you can use the model to make predictions:', 34: 'The first plant is classified as an Iris virginica, while the second is not', 35: 'Let’s look at the scores that the SVM used\nto make these predictions', 36: 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 37: 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 38: 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 39: 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 41: 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 42: 'This dataset is not linearly separable, as\nyou can see', 43: 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 44: 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 46: 'You can generate this dataset using the \n function:\n1\n2\n1\n2'}
2025-01-18 15:54:57,664 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-18 15:55:00,741 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-18 15:55:00,747 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    18,
    39
  ],
  "Examples": [
    6,
    20,
    31,
    45
  ],
  "Definitions": [
    1,
    12,
    14,
    19,
    22
  ]
}
```
2025-01-18 15:55:00,758 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-18 15:55:00,758 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-18 15:55:00,784 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 1
2025-01-18 15:55:00,784 - src.functions - DEBUG - Found rects: [Rect(75.328125, 182.0888671875, 438.5721130371094, 193.1630859375)]
2025-01-18 15:55:00,792 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 1
2025-01-18 15:55:00,792 - src.functions - DEBUG - Found rects: [Rect(443.5721130371094, 182.0888671875, 521.6256103515625, 193.1630859375), Rect(75.328125, 214.1768035888672, 117.83416748046875, 230.00177001953125), Rect(121.76940155029297, 214.1768035888672, 152.46279907226562, 230.00177001953125), Rect(156.39804077148438, 214.1768035888672, 248.49273681640625, 230.00177001953125), Rect(75.328125, 236.5888671875, 363.6017150878906, 247.6630859375)]
2025-01-18 15:55:00,806 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-18 15:55:00,806 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,806 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-18 15:55:00,809 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1
2025-01-18 15:55:00,809 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,809 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1.
2025-01-18 15:55:00,813 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 1
2025-01-18 15:55:00,813 - src.functions - DEBUG - Found rects: [Rect(368.6094055175781, 236.5888671875, 527.1851806640625, 247.6630859375), Rect(75.328125, 249.0888671875, 247.49119567871094, 260.1630859375)]
2025-01-18 15:55:00,819 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1
2025-01-18 15:55:00,819 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,819 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1.
2025-01-18 15:55:00,821 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1
2025-01-18 15:55:00,822 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,822 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1.
2025-01-18 15:55:00,824 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1
2025-01-18 15:55:00,824 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,824 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1.
2025-01-18 15:55:00,827 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-18 15:55:00,827 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-18 15:55:00,836 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 1
2025-01-18 15:55:00,836 - src.functions - DEBUG - Found rects: [Rect(415.2421875, 324.0888671875, 517.8462524414062, 335.1630859375), Rect(75.328125, 336.5888671875, 499.3355712890625, 347.6630859375)]
2025-01-18 15:55:00,841 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 1
2025-01-18 15:55:00,842 - src.functions - DEBUG - Found rects: [Rect(389.6484375, 468.5888671875, 508.47076416015625, 479.6630859375), Rect(75.328125, 481.0888671875, 262.767578125, 492.1630859375)]
2025-01-18 15:55:00,846 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 1
2025-01-18 15:55:00,846 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,846 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 1.
2025-01-18 15:55:00,847 - src.functions - DEBUG - Searching for sentence #22: 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 1
2025-01-18 15:55:00,847 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,847 - src.functions - WARNING - Could not find sentence 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 1.
2025-01-18 15:55:00,854 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-18 15:55:00,854 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,854 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-18 15:55:00,861 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2
2025-01-18 15:55:00,861 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,861 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2.
2025-01-18 15:55:00,868 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2
2025-01-18 15:55:00,868 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,868 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2.
2025-01-18 15:55:00,875 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-18 15:55:00,875 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-18 15:55:00,883 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2
2025-01-18 15:55:00,883 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,883 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2.
2025-01-18 15:55:00,890 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2
2025-01-18 15:55:00,890 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,890 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2.
2025-01-18 15:55:00,897 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 2
2025-01-18 15:55:00,897 - src.functions - DEBUG - Found rects: [Rect(252.51565551757812, 97.5888671875, 522.7468872070312, 108.6630859375), Rect(75.328125, 110.0888671875, 527.3569946289062, 121.1630859375), Rect(75.328125, 122.5888671875, 459.814208984375, 133.6630859375)]
2025-01-18 15:55:00,907 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 2
2025-01-18 15:55:00,907 - src.functions - DEBUG - Found rects: [Rect(75.328125, 522.0888671875, 517.880615234375, 533.1630859375), Rect(75.328125, 535.5888671875, 105.3177261352539, 546.6630859375)]
2025-01-18 15:55:00,916 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2
2025-01-18 15:55:00,916 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,916 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2.
2025-01-18 15:55:00,923 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-18 15:55:00,923 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,923 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-18 15:55:00,929 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2
2025-01-18 15:55:00,929 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,929 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2.
2025-01-18 15:55:00,936 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2
2025-01-18 15:55:00,936 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,936 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2.
2025-01-18 15:55:00,943 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 2
2025-01-18 15:55:00,943 - src.functions - DEBUG - Found rects: [Rect(369.5124206542969, 85.0888671875, 530.3182373046875, 96.1630859375), Rect(75.328125, 97.5888671875, 113.08060455322266, 108.6630859375)]
2025-01-18 15:55:00,952 - src.functions - DEBUG - Searching for sentence #22: 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 2
2025-01-18 15:55:00,952 - src.functions - DEBUG - Found rects: [Rect(319.5546875, 234.0888671875, 510.58648681640625, 245.1630859375), Rect(75.328125, 246.5888671875, 372.4911193847656, 257.6630859375)]
2025-01-18 15:55:00,963 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-18 15:55:00,963 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,963 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-18 15:55:00,972 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3
2025-01-18 15:55:00,972 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,972 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3.
2025-01-18 15:55:00,980 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3
2025-01-18 15:55:00,980 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,980 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3.
2025-01-18 15:55:00,989 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-18 15:55:00,989 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:00,989 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-18 15:55:00,998 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 3
2025-01-18 15:55:00,998 - src.functions - DEBUG - Found rects: [Rect(75.328125, 292.1767883300781, 140.65020751953125, 308.00177001953125), Rect(144.58544921875, 292.1767883300781, 175.27883911132812, 308.00177001953125), Rect(179.214111328125, 292.1767883300781, 271.3088073730469, 308.00177001953125), Rect(75.328125, 314.5888671875, 534.448974609375, 325.6630859375), Rect(75.328125, 327.0888671875, 170.83660888671875, 338.1630859375)]
2025-01-18 15:55:01,011 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3
2025-01-18 15:55:01,011 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:01,011 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3.
2025-01-18 15:55:01,019 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3
2025-01-18 15:55:01,019 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:01,019 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3.
2025-01-18 15:55:01,028 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3
2025-01-18 15:55:01,028 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:01,028 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3.
2025-01-18 15:55:01,037 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 3
2025-01-18 15:55:01,037 - src.functions - DEBUG - Found rects: [Rect(75.328125, 525.5888671875, 521.5821533203125, 536.6630859375), Rect(75.328125, 539.0888671875, 271.3769836425781, 550.1630859375)]
2025-01-18 15:55:01,047 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-18 15:55:01,047 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:01,047 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-18 15:55:01,056 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3
2025-01-18 15:55:01,056 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:01,056 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3.
2025-01-18 15:55:01,064 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3
2025-01-18 15:55:01,064 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:01,064 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3.
2025-01-18 15:55:01,073 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 3
2025-01-18 15:55:01,073 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:01,073 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 3.
2025-01-18 15:55:01,081 - src.functions - DEBUG - Searching for sentence #22: 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 3
2025-01-18 15:55:01,082 - src.functions - DEBUG - Found rects: []
2025-01-18 15:55:01,082 - src.functions - WARNING - Could not find sentence 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 3.
2025-01-19 00:03:19,592 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 00:03:19,594 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 00:03:19,594 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 00:03:19,594 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 00:03:19,595 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 00:03:19,595 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 00:03:19,660 - src.functions - DEBUG - Sentences: ['Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', ', hundreds to thousands of instances), especially for classification tasks', 'However, they\ndon’t scale very well to very large datasets, as you will see', 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 'The left plot shows the decision boundaries of three possible linear classifiers', 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 'First, it only works if the data is linearly\nseparable', 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', ', instances that end up in the middle of\nthe street or even on the wrong side)', 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 'With a high value, you get the model on the right', 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 'Then, as usual, you can use the model to make predictions:', 'The first plant is classified as an Iris virginica, while the second is not', 'Let’s look at the scores that the SVM used\nto make these predictions', 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 'This dataset is not linearly separable, as\nyou can see', 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 'You can generate this dataset using the \n function:\n1\n2\n1\n2']
2025-01-19 00:03:19,660 - src.functions - DEBUG - Numbered Sentences: {1: 'Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', 2: ', hundreds to thousands of instances), especially for classification tasks', 3: 'However, they\ndon’t scale very well to very large datasets, as you will see', 4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 5: 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 6: 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 7: 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 8: 'The left plot shows the decision boundaries of three possible linear classifiers', 9: 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 10: 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 11: 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 12: 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 13: 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 14: 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 15: 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 16: 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', 17: ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 19: 'First, it only works if the data is linearly\nseparable', 20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 21: 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 22: 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', 23: ', instances that end up in the middle of\nthe street or even on the wrong side)', 24: 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 25: 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 26: 'With a high value, you get the model on the right', 27: 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 28: 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 29: 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 30: 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 32: 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 33: 'Then, as usual, you can use the model to make predictions:', 34: 'The first plant is classified as an Iris virginica, while the second is not', 35: 'Let’s look at the scores that the SVM used\nto make these predictions', 36: 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 37: 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 38: 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 39: 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 41: 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 42: 'This dataset is not linearly separable, as\nyou can see', 43: 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 44: 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 46: 'You can generate this dataset using the \n function:\n1\n2\n1\n2'}
2025-01-19 00:03:19,918 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-19 00:03:23,561 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-19 00:03:23,564 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    11,
    18,
    39
  ],
  "Examples": [
    6,
    20,
    31,
    45
  ],
  "Definitions": [
    1,
    12,
    14,
    18,
    40,
    44
  ]
}
```
2025-01-19 00:03:23,576 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-19 00:03:23,577 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-19 00:03:23,606 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 1
2025-01-19 00:03:23,606 - src.functions - DEBUG - Found rects: [Rect(75.328125, 182.0888671875, 438.5721130371094, 193.1630859375)]
2025-01-19 00:03:23,613 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 1
2025-01-19 00:03:23,613 - src.functions - DEBUG - Found rects: [Rect(443.5721130371094, 182.0888671875, 521.6256103515625, 193.1630859375), Rect(75.328125, 214.1768035888672, 117.83416748046875, 230.00177001953125), Rect(121.76940155029297, 214.1768035888672, 152.46279907226562, 230.00177001953125), Rect(156.39804077148438, 214.1768035888672, 248.49273681640625, 230.00177001953125), Rect(75.328125, 236.5888671875, 363.6017150878906, 247.6630859375)]
2025-01-19 00:03:23,627 - src.functions - DEBUG - Searching for sentence #11: 'In contrast, the solid
line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the
two classes but also stays as far away from the closest training instances as possible' on page 1
2025-01-19 00:03:23,627 - src.functions - DEBUG - Found rects: [Rect(444.39410400390625, 299.0888671875, 526.0419921875, 310.1630859375), Rect(75.328125, 311.5888671875, 527.990478515625, 322.6630859375), Rect(75.328125, 324.0888671875, 410.24267578125, 335.1630859375)]
2025-01-19 00:03:23,634 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-19 00:03:23,634 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,634 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-19 00:03:23,637 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1
2025-01-19 00:03:23,637 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,637 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1.
2025-01-19 00:03:23,640 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 1
2025-01-19 00:03:23,640 - src.functions - DEBUG - Found rects: [Rect(368.6094055175781, 236.5888671875, 527.1851806640625, 247.6630859375), Rect(75.328125, 249.0888671875, 247.49119567871094, 260.1630859375)]
2025-01-19 00:03:23,645 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1
2025-01-19 00:03:23,645 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,645 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1.
2025-01-19 00:03:23,648 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1
2025-01-19 00:03:23,648 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,648 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1.
2025-01-19 00:03:23,650 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1
2025-01-19 00:03:23,650 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,650 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1.
2025-01-19 00:03:23,652 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-19 00:03:23,652 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-19 00:03:23,660 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 1
2025-01-19 00:03:23,660 - src.functions - DEBUG - Found rects: [Rect(415.2421875, 324.0888671875, 517.8462524414062, 335.1630859375), Rect(75.328125, 336.5888671875, 499.3355712890625, 347.6630859375)]
2025-01-19 00:03:23,665 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 1
2025-01-19 00:03:23,665 - src.functions - DEBUG - Found rects: [Rect(389.6484375, 468.5888671875, 508.47076416015625, 479.6630859375), Rect(75.328125, 481.0888671875, 262.767578125, 492.1630859375)]
2025-01-19 00:03:23,669 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-19 00:03:23,669 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,669 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-19 00:03:23,670 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1
2025-01-19 00:03:23,671 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,671 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1.
2025-01-19 00:03:23,672 - src.functions - DEBUG - Searching for sentence #44: 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 1
2025-01-19 00:03:23,672 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,672 - src.functions - WARNING - Could not find sentence 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 1.
2025-01-19 00:03:23,679 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-19 00:03:23,679 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,679 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-19 00:03:23,686 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2
2025-01-19 00:03:23,686 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,686 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2.
2025-01-19 00:03:23,693 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2
2025-01-19 00:03:23,693 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,693 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2.
2025-01-19 00:03:23,699 - src.functions - DEBUG - Searching for sentence #11: 'In contrast, the solid
line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the
two classes but also stays as far away from the closest training instances as possible' on page 2
2025-01-19 00:03:23,699 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,699 - src.functions - WARNING - Could not find sentence 'In contrast, the solid
line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the
two classes but also stays as far away from the closest training instances as possible' on page 2.
2025-01-19 00:03:23,706 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-19 00:03:23,706 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-19 00:03:23,715 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2
2025-01-19 00:03:23,715 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,715 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2.
2025-01-19 00:03:23,722 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2
2025-01-19 00:03:23,722 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,722 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2.
2025-01-19 00:03:23,729 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 2
2025-01-19 00:03:23,729 - src.functions - DEBUG - Found rects: [Rect(252.51565551757812, 97.5888671875, 522.7468872070312, 108.6630859375), Rect(75.328125, 110.0888671875, 527.3569946289062, 121.1630859375), Rect(75.328125, 122.5888671875, 459.814208984375, 133.6630859375)]
2025-01-19 00:03:23,739 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 2
2025-01-19 00:03:23,739 - src.functions - DEBUG - Found rects: [Rect(75.328125, 522.0888671875, 517.880615234375, 533.1630859375), Rect(75.328125, 535.5888671875, 105.3177261352539, 546.6630859375)]
2025-01-19 00:03:23,748 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2
2025-01-19 00:03:23,748 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,748 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2.
2025-01-19 00:03:23,754 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-19 00:03:23,754 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,754 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-19 00:03:23,761 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2
2025-01-19 00:03:23,761 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,761 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2.
2025-01-19 00:03:23,768 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2
2025-01-19 00:03:23,768 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,768 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2.
2025-01-19 00:03:23,775 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-19 00:03:23,775 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-19 00:03:23,783 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2
2025-01-19 00:03:23,784 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,784 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2.
2025-01-19 00:03:23,790 - src.functions - DEBUG - Searching for sentence #44: 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 2
2025-01-19 00:03:23,790 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,790 - src.functions - WARNING - Could not find sentence 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 2.
2025-01-19 00:03:23,799 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-19 00:03:23,799 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,799 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-19 00:03:23,807 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3
2025-01-19 00:03:23,807 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,807 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3.
2025-01-19 00:03:23,816 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3
2025-01-19 00:03:23,816 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,816 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3.
2025-01-19 00:03:23,824 - src.functions - DEBUG - Searching for sentence #11: 'In contrast, the solid
line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the
two classes but also stays as far away from the closest training instances as possible' on page 3
2025-01-19 00:03:23,824 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,824 - src.functions - WARNING - Could not find sentence 'In contrast, the solid
line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the
two classes but also stays as far away from the closest training instances as possible' on page 3.
2025-01-19 00:03:23,833 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-19 00:03:23,833 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,833 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-19 00:03:23,842 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 3
2025-01-19 00:03:23,842 - src.functions - DEBUG - Found rects: [Rect(75.328125, 292.1767883300781, 140.65020751953125, 308.00177001953125), Rect(144.58544921875, 292.1767883300781, 175.27883911132812, 308.00177001953125), Rect(179.214111328125, 292.1767883300781, 271.3088073730469, 308.00177001953125), Rect(75.328125, 314.5888671875, 534.448974609375, 325.6630859375), Rect(75.328125, 327.0888671875, 170.83660888671875, 338.1630859375)]
2025-01-19 00:03:23,855 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3
2025-01-19 00:03:23,855 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,855 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3.
2025-01-19 00:03:23,863 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3
2025-01-19 00:03:23,863 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,863 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3.
2025-01-19 00:03:23,872 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3
2025-01-19 00:03:23,872 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,872 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3.
2025-01-19 00:03:23,881 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 3
2025-01-19 00:03:23,881 - src.functions - DEBUG - Found rects: [Rect(75.328125, 525.5888671875, 521.5821533203125, 536.6630859375), Rect(75.328125, 539.0888671875, 271.3769836425781, 550.1630859375)]
2025-01-19 00:03:23,891 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-19 00:03:23,891 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,891 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-19 00:03:23,900 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3
2025-01-19 00:03:23,900 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,900 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3.
2025-01-19 00:03:23,909 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3
2025-01-19 00:03:23,909 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,909 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3.
2025-01-19 00:03:23,917 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-19 00:03:23,917 - src.functions - DEBUG - Found rects: []
2025-01-19 00:03:23,917 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-19 00:03:23,926 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 3
2025-01-19 00:03:23,926 - src.functions - DEBUG - Found rects: [Rect(175.83595275878906, 327.0888671875, 529.6141357421875, 338.1630859375), Rect(75.328125, 339.5888671875, 444.8943786621094, 350.6630859375)]
2025-01-19 00:03:23,937 - src.functions - DEBUG - Searching for sentence #44: 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 3
2025-01-19 00:03:23,937 - src.functions - DEBUG - Found rects: [Rect(244.54690551757812, 481.816650390625, 403.197998046875, 490.122314453125), Rect(75.328125, 499.5888671875, 402.28619384765625, 510.6630859375), Rect(75.328125, 513.0888671875, 346.64215087890625, 524.1630859375), Rect(416.6328125, 513.0888671875, 443.0087890625, 524.1630859375), Rect(488.0, 513.0888671875, 526.592041015625, 524.1630859375)]
2025-01-19 08:33:18,843 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 08:33:18,845 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 08:33:18,845 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 08:33:18,847 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 08:33:18,847 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 08:33:18,847 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 08:33:19,252 - src.functions - DEBUG - Sentences: ['Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', ', hundreds to thousands of instances), especially for classification tasks', 'However, they\ndon’t scale very well to very large datasets, as you will see', 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 'The left plot shows the decision boundaries of three possible linear classifiers', 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 'First, it only works if the data is linearly\nseparable', 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', ', instances that end up in the middle of\nthe street or even on the wrong side)', 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 'With a high value, you get the model on the right', 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 'Then, as usual, you can use the model to make predictions:', 'The first plant is classified as an Iris virginica, while the second is not', 'Let’s look at the scores that the SVM used\nto make these predictions', 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 'This dataset is not linearly separable, as\nyou can see', 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 'You can generate this dataset using the \n function:\n1\n2\n1\n2']
2025-01-19 08:33:19,253 - src.functions - DEBUG - Numbered Sentences: {1: 'Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', 2: ', hundreds to thousands of instances), especially for classification tasks', 3: 'However, they\ndon’t scale very well to very large datasets, as you will see', 4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 5: 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 6: 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 7: 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 8: 'The left plot shows the decision boundaries of three possible linear classifiers', 9: 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 10: 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 11: 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 12: 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 13: 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 14: 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 15: 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 16: 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', 17: ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 19: 'First, it only works if the data is linearly\nseparable', 20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 21: 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 22: 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', 23: ', instances that end up in the middle of\nthe street or even on the wrong side)', 24: 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 25: 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 26: 'With a high value, you get the model on the right', 27: 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 28: 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 29: 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 30: 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 32: 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 33: 'Then, as usual, you can use the model to make predictions:', 34: 'The first plant is classified as an Iris virginica, while the second is not', 35: 'Let’s look at the scores that the SVM used\nto make these predictions', 36: 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 37: 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 38: 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 39: 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 41: 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 42: 'This dataset is not linearly separable, as\nyou can see', 43: 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 44: 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 46: 'You can generate this dataset using the \n function:\n1\n2\n1\n2'}
2025-01-19 08:33:19,432 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-19 08:33:23,784 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-19 08:33:23,787 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    12,
    18,
    21,
    39
  ],
  "Examples": [
    6,
    20,
    31,
    45
  ],
  "Definitions": [
    1,
    14,
    15,
    19,
    22,
    28
  ]
}
```
2025-01-19 08:33:23,794 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-19 08:33:23,794 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-19 08:33:23,821 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 1
2025-01-19 08:33:23,822 - src.functions - DEBUG - Found rects: [Rect(75.328125, 182.0888671875, 438.5721130371094, 193.1630859375)]
2025-01-19 08:33:23,830 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 1
2025-01-19 08:33:23,830 - src.functions - DEBUG - Found rects: [Rect(443.5721130371094, 182.0888671875, 521.6256103515625, 193.1630859375), Rect(75.328125, 214.1768035888672, 117.83416748046875, 230.00177001953125), Rect(121.76940155029297, 214.1768035888672, 152.46279907226562, 230.00177001953125), Rect(156.39804077148438, 214.1768035888672, 248.49273681640625, 230.00177001953125), Rect(75.328125, 236.5888671875, 363.6017150878906, 247.6630859375)]
2025-01-19 08:33:23,845 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 1
2025-01-19 08:33:23,845 - src.functions - DEBUG - Found rects: [Rect(415.2421875, 324.0888671875, 517.8462524414062, 335.1630859375), Rect(75.328125, 336.5888671875, 499.3355712890625, 347.6630859375)]
2025-01-19 08:33:23,851 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-19 08:33:23,851 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,851 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-19 08:33:23,854 - src.functions - DEBUG - Searching for sentence #21: 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 1
2025-01-19 08:33:23,854 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,854 - src.functions - WARNING - Could not find sentence 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 1.
2025-01-19 08:33:23,857 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1
2025-01-19 08:33:23,857 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,857 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1.
2025-01-19 08:33:23,860 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 1
2025-01-19 08:33:23,860 - src.functions - DEBUG - Found rects: [Rect(368.6094055175781, 236.5888671875, 527.1851806640625, 247.6630859375), Rect(75.328125, 249.0888671875, 247.49119567871094, 260.1630859375)]
2025-01-19 08:33:23,865 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1
2025-01-19 08:33:23,865 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,865 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1.
2025-01-19 08:33:23,868 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1
2025-01-19 08:33:23,868 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,868 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1.
2025-01-19 08:33:23,870 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1
2025-01-19 08:33:23,870 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,870 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1.
2025-01-19 08:33:23,873 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-19 08:33:23,873 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-19 08:33:23,881 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 1
2025-01-19 08:33:23,881 - src.functions - DEBUG - Found rects: [Rect(389.6484375, 468.5888671875, 508.47076416015625, 479.6630859375), Rect(75.328125, 481.0888671875, 262.767578125, 492.1630859375)]
2025-01-19 08:33:23,886 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 1
2025-01-19 08:33:23,886 - src.functions - DEBUG - Found rects: [Rect(280.1015625, 515.4517822265625, 331.3959655761719, 526.6181030273438), Rect(94.828125, 532.816650390625, 302.26629638671875, 541.122314453125)]
2025-01-19 08:33:23,889 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 1
2025-01-19 08:33:23,889 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,889 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 1.
2025-01-19 08:33:23,891 - src.functions - DEBUG - Searching for sentence #22: 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 1
2025-01-19 08:33:23,891 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,891 - src.functions - WARNING - Could not find sentence 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 1.
2025-01-19 08:33:23,893 - src.functions - DEBUG - Searching for sentence #28: 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 1
2025-01-19 08:33:23,893 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,893 - src.functions - WARNING - Could not find sentence 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 1.
2025-01-19 08:33:23,900 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-19 08:33:23,900 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,900 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-19 08:33:23,907 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2
2025-01-19 08:33:23,907 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,907 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2.
2025-01-19 08:33:23,913 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2
2025-01-19 08:33:23,913 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,913 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2.
2025-01-19 08:33:23,920 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2
2025-01-19 08:33:23,920 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,920 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2.
2025-01-19 08:33:23,927 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-19 08:33:23,927 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-19 08:33:23,936 - src.functions - DEBUG - Searching for sentence #21: 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 2
2025-01-19 08:33:23,936 - src.functions - DEBUG - Found rects: [Rect(272.28125, 217.316650390625, 375.4692077636719, 225.622314453125), Rect(75.328125, 234.0888671875, 314.55462646484375, 245.1630859375)]
2025-01-19 08:33:23,944 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2
2025-01-19 08:33:23,944 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,944 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2.
2025-01-19 08:33:23,951 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2
2025-01-19 08:33:23,951 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,951 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2.
2025-01-19 08:33:23,958 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 2
2025-01-19 08:33:23,958 - src.functions - DEBUG - Found rects: [Rect(252.51565551757812, 97.5888671875, 522.7468872070312, 108.6630859375), Rect(75.328125, 110.0888671875, 527.3569946289062, 121.1630859375), Rect(75.328125, 122.5888671875, 459.814208984375, 133.6630859375)]
2025-01-19 08:33:23,968 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 2
2025-01-19 08:33:23,968 - src.functions - DEBUG - Found rects: [Rect(75.328125, 522.0888671875, 517.880615234375, 533.1630859375), Rect(75.328125, 535.5888671875, 105.3177261352539, 546.6630859375)]
2025-01-19 08:33:23,977 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2
2025-01-19 08:33:23,977 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,977 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2.
2025-01-19 08:33:23,984 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-19 08:33:23,984 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,984 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-19 08:33:23,991 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2
2025-01-19 08:33:23,991 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,991 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2.
2025-01-19 08:33:23,997 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 2
2025-01-19 08:33:23,997 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:23,998 - src.functions - WARNING - Could not find sentence 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 2.
2025-01-19 08:33:24,004 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 2
2025-01-19 08:33:24,004 - src.functions - DEBUG - Found rects: [Rect(369.5124206542969, 85.0888671875, 530.3182373046875, 96.1630859375), Rect(75.328125, 97.5888671875, 113.08060455322266, 108.6630859375)]
2025-01-19 08:33:24,013 - src.functions - DEBUG - Searching for sentence #22: 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 2
2025-01-19 08:33:24,013 - src.functions - DEBUG - Found rects: [Rect(319.5546875, 234.0888671875, 510.58648681640625, 245.1630859375), Rect(75.328125, 246.5888671875, 372.4911193847656, 257.6630859375)]
2025-01-19 08:33:24,023 - src.functions - DEBUG - Searching for sentence #28: 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 2
2025-01-19 08:33:24,023 - src.functions - DEBUG - Found rects: [Rect(248.72975158691406, 317.0888671875, 349.5337219238281, 328.1630859375), Rect(354.5234375, 317.0888671875, 515.6038208007812, 328.1630859375), Rect(75.328125, 329.5888671875, 230.57064819335938, 340.6630859375)]
2025-01-19 08:33:24,034 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-19 08:33:24,034 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:24,034 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-19 08:33:24,043 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3
2025-01-19 08:33:24,043 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:24,043 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3.
2025-01-19 08:33:24,051 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3
2025-01-19 08:33:24,051 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:24,051 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3.
2025-01-19 08:33:24,059 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3
2025-01-19 08:33:24,059 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:24,060 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3.
2025-01-19 08:33:24,068 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-19 08:33:24,068 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:24,068 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-19 08:33:24,076 - src.functions - DEBUG - Searching for sentence #21: 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 3
2025-01-19 08:33:24,076 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:24,076 - src.functions - WARNING - Could not find sentence 'Hard margin sensitivity to outliers
To avoid these issues, we need to use a more flexible model' on page 3.
2025-01-19 08:33:24,085 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 3
2025-01-19 08:33:24,085 - src.functions - DEBUG - Found rects: [Rect(75.328125, 292.1767883300781, 140.65020751953125, 308.00177001953125), Rect(144.58544921875, 292.1767883300781, 175.27883911132812, 308.00177001953125), Rect(179.214111328125, 292.1767883300781, 271.3088073730469, 308.00177001953125), Rect(75.328125, 314.5888671875, 534.448974609375, 325.6630859375), Rect(75.328125, 327.0888671875, 170.83660888671875, 338.1630859375)]
2025-01-19 08:33:24,098 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3
2025-01-19 08:33:24,098 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:24,098 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3.
2025-01-19 08:33:24,107 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3
2025-01-19 08:33:24,107 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:24,107 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3.
2025-01-19 08:33:24,115 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3
2025-01-19 08:33:24,115 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:24,115 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3.
2025-01-19 08:33:24,125 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 3
2025-01-19 08:33:24,125 - src.functions - DEBUG - Found rects: [Rect(75.328125, 525.5888671875, 521.5821533203125, 536.6630859375), Rect(75.328125, 539.0888671875, 271.3769836425781, 550.1630859375)]
2025-01-19 08:33:24,135 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-19 08:33:24,135 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:24,135 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-19 08:33:24,143 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3
2025-01-19 08:33:24,143 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:24,143 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3.
2025-01-19 08:33:24,152 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 3
2025-01-19 08:33:24,152 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:24,152 - src.functions - WARNING - Could not find sentence 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 3.
2025-01-19 08:33:24,160 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 3
2025-01-19 08:33:24,160 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:24,160 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 3.
2025-01-19 08:33:24,169 - src.functions - DEBUG - Searching for sentence #22: 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 3
2025-01-19 08:33:24,169 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:24,169 - src.functions - WARNING - Could not find sentence 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 3.
2025-01-19 08:33:24,177 - src.functions - DEBUG - Searching for sentence #28: 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 3
2025-01-19 08:33:24,177 - src.functions - DEBUG - Found rects: []
2025-01-19 08:33:24,177 - src.functions - WARNING - Could not find sentence 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 3.
2025-01-19 09:11:09,286 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:11:09,287 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:11:09,287 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:11:09,287 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:11:09,287 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:11:09,287 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:11:09,396 - src.functions - DEBUG - Sentences: ['Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', ', hundreds to thousands of instances), especially for classification tasks', 'However, they\ndon’t scale very well to very large datasets, as you will see', 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 'The left plot shows the decision boundaries of three possible linear classifiers', 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 'First, it only works if the data is linearly\nseparable', 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', ', instances that end up in the middle of\nthe street or even on the wrong side)', 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 'With a high value, you get the model on the right', 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 'Then, as usual, you can use the model to make predictions:', 'The first plant is classified as an Iris virginica, while the second is not', 'Let’s look at the scores that the SVM used\nto make these predictions', 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 'This dataset is not linearly separable, as\nyou can see', 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 'You can generate this dataset using the \n function:\n1\n2\n1\n2']
2025-01-19 09:11:09,398 - src.functions - DEBUG - Numbered Sentences: {1: 'Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', 2: ', hundreds to thousands of instances), especially for classification tasks', 3: 'However, they\ndon’t scale very well to very large datasets, as you will see', 4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 5: 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 6: 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 7: 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 8: 'The left plot shows the decision boundaries of three possible linear classifiers', 9: 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 10: 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 11: 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 12: 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 13: 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 14: 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 15: 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 16: 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', 17: ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 19: 'First, it only works if the data is linearly\nseparable', 20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 21: 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 22: 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', 23: ', instances that end up in the middle of\nthe street or even on the wrong side)', 24: 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 25: 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 26: 'With a high value, you get the model on the right', 27: 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 28: 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 29: 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 30: 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 32: 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 33: 'Then, as usual, you can use the model to make predictions:', 34: 'The first plant is classified as an Iris virginica, while the second is not', 35: 'Let’s look at the scores that the SVM used\nto make these predictions', 36: 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 37: 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 38: 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 39: 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 41: 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 42: 'This dataset is not linearly separable, as\nyou can see', 43: 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 44: 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 46: 'You can generate this dataset using the \n function:\n1\n2\n1\n2'}
2025-01-19 09:11:09,808 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-19 09:11:13,259 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-19 09:11:13,261 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    39,
    40,
     44
  ],
  "Examples": [
    6,
    20,
    31,
    45
  ],
  "Definitions": [
    1,
    12,
    14,
    18,
    19,
    22,
    27,
    28,
    36
  ]
}
```
2025-01-19 09:11:13,270 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-19 09:11:13,271 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-19 09:11:13,295 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 1
2025-01-19 09:11:13,295 - src.functions - DEBUG - Found rects: [Rect(75.328125, 182.0888671875, 438.5721130371094, 193.1630859375)]
2025-01-19 09:11:13,304 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 1
2025-01-19 09:11:13,304 - src.functions - DEBUG - Found rects: [Rect(443.5721130371094, 182.0888671875, 521.6256103515625, 193.1630859375), Rect(75.328125, 214.1768035888672, 117.83416748046875, 230.00177001953125), Rect(121.76940155029297, 214.1768035888672, 152.46279907226562, 230.00177001953125), Rect(156.39804077148438, 214.1768035888672, 248.49273681640625, 230.00177001953125), Rect(75.328125, 236.5888671875, 363.6017150878906, 247.6630859375)]
2025-01-19 09:11:13,317 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1
2025-01-19 09:11:13,317 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,317 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1.
2025-01-19 09:11:13,320 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1
2025-01-19 09:11:13,320 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,320 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1.
2025-01-19 09:11:13,323 - src.functions - DEBUG - Searching for sentence #44: 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 1
2025-01-19 09:11:13,323 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,323 - src.functions - WARNING - Could not find sentence 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 1.
2025-01-19 09:11:13,327 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 1
2025-01-19 09:11:13,327 - src.functions - DEBUG - Found rects: [Rect(368.6094055175781, 236.5888671875, 527.1851806640625, 247.6630859375), Rect(75.328125, 249.0888671875, 247.49119567871094, 260.1630859375)]
2025-01-19 09:11:13,333 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1
2025-01-19 09:11:13,333 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,333 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1.
2025-01-19 09:11:13,335 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1
2025-01-19 09:11:13,335 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,335 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1.
2025-01-19 09:11:13,338 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1
2025-01-19 09:11:13,338 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,338 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1.
2025-01-19 09:11:13,341 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-19 09:11:13,341 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-19 09:11:13,351 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 1
2025-01-19 09:11:13,351 - src.functions - DEBUG - Found rects: [Rect(415.2421875, 324.0888671875, 517.8462524414062, 335.1630859375), Rect(75.328125, 336.5888671875, 499.3355712890625, 347.6630859375)]
2025-01-19 09:11:13,356 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 1
2025-01-19 09:11:13,356 - src.functions - DEBUG - Found rects: [Rect(389.6484375, 468.5888671875, 508.47076416015625, 479.6630859375), Rect(75.328125, 481.0888671875, 262.767578125, 492.1630859375)]
2025-01-19 09:11:13,360 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-19 09:11:13,360 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,360 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-19 09:11:13,362 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 1
2025-01-19 09:11:13,362 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,362 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 1.
2025-01-19 09:11:13,364 - src.functions - DEBUG - Searching for sentence #22: 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 1
2025-01-19 09:11:13,364 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,364 - src.functions - WARNING - Could not find sentence 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 1.
2025-01-19 09:11:13,366 - src.functions - DEBUG - Searching for sentence #27: 'As you can see, reducing  makes the street larger,
but it also leads to more margin violations' on page 1
2025-01-19 09:11:13,366 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,366 - src.functions - WARNING - Could not find sentence 'As you can see, reducing  makes the street larger,
but it also leads to more margin violations' on page 1.
2025-01-19 09:11:13,368 - src.functions - DEBUG - Searching for sentence #28: 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 1
2025-01-19 09:11:13,368 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,368 - src.functions - WARNING - Could not find sentence 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 1.
2025-01-19 09:11:13,370 - src.functions - DEBUG - Searching for sentence #36: 'These measure the signed distance between each instance and the decision boundary:
Unlike 
, 
 doesn’t have a 
 method to estimate the class
probabilities' on page 1
2025-01-19 09:11:13,370 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,370 - src.functions - WARNING - Could not find sentence 'These measure the signed distance between each instance and the decision boundary:
Unlike 
, 
 doesn’t have a 
 method to estimate the class
probabilities' on page 1.
2025-01-19 09:11:13,377 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-19 09:11:13,377 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,377 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-19 09:11:13,384 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2
2025-01-19 09:11:13,384 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,384 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2.
2025-01-19 09:11:13,390 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2
2025-01-19 09:11:13,390 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,391 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2.
2025-01-19 09:11:13,397 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2
2025-01-19 09:11:13,397 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,397 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2.
2025-01-19 09:11:13,404 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2
2025-01-19 09:11:13,404 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,404 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2.
2025-01-19 09:11:13,411 - src.functions - DEBUG - Searching for sentence #44: 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 2
2025-01-19 09:11:13,411 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,411 - src.functions - WARNING - Could not find sentence 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 2.
2025-01-19 09:11:13,417 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2
2025-01-19 09:11:13,418 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,418 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2.
2025-01-19 09:11:13,425 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 2
2025-01-19 09:11:13,425 - src.functions - DEBUG - Found rects: [Rect(252.51565551757812, 97.5888671875, 522.7468872070312, 108.6630859375), Rect(75.328125, 110.0888671875, 527.3569946289062, 121.1630859375), Rect(75.328125, 122.5888671875, 459.814208984375, 133.6630859375)]
2025-01-19 09:11:13,435 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 2
2025-01-19 09:11:13,435 - src.functions - DEBUG - Found rects: [Rect(75.328125, 522.0888671875, 517.880615234375, 533.1630859375), Rect(75.328125, 535.5888671875, 105.3177261352539, 546.6630859375)]
2025-01-19 09:11:13,443 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2
2025-01-19 09:11:13,443 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,444 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2.
2025-01-19 09:11:13,450 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-19 09:11:13,450 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,450 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-19 09:11:13,457 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2
2025-01-19 09:11:13,457 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,457 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2.
2025-01-19 09:11:13,464 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2
2025-01-19 09:11:13,464 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,464 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2.
2025-01-19 09:11:13,471 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-19 09:11:13,471 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-19 09:11:13,480 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 2
2025-01-19 09:11:13,480 - src.functions - DEBUG - Found rects: [Rect(369.5124206542969, 85.0888671875, 530.3182373046875, 96.1630859375), Rect(75.328125, 97.5888671875, 113.08060455322266, 108.6630859375)]
2025-01-19 09:11:13,489 - src.functions - DEBUG - Searching for sentence #22: 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 2
2025-01-19 09:11:13,489 - src.functions - DEBUG - Found rects: [Rect(319.5546875, 234.0888671875, 510.58648681640625, 245.1630859375), Rect(75.328125, 246.5888671875, 372.4911193847656, 257.6630859375)]
2025-01-19 09:11:13,498 - src.functions - DEBUG - Searching for sentence #27: 'As you can see, reducing  makes the street larger,
but it also leads to more margin violations' on page 2
2025-01-19 09:11:13,498 - src.functions - DEBUG - Found rects: [Rect(323.7577209472656, 303.5888671875, 426.49969482421875, 314.6630859375), Rect(431.5, 303.5888671875, 526.1533203125, 314.6630859375), Rect(75.328125, 317.0888671875, 243.72935485839844, 328.1630859375)]
2025-01-19 09:11:13,508 - src.functions - DEBUG - Searching for sentence #28: 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 2
2025-01-19 09:11:13,508 - src.functions - DEBUG - Found rects: [Rect(248.72975158691406, 317.0888671875, 349.5337219238281, 328.1630859375), Rect(354.5234375, 317.0888671875, 515.6038208007812, 328.1630859375), Rect(75.328125, 329.5888671875, 230.57064819335938, 340.6630859375)]
2025-01-19 09:11:13,517 - src.functions - DEBUG - Searching for sentence #36: 'These measure the signed distance between each instance and the decision boundary:
Unlike 
, 
 doesn’t have a 
 method to estimate the class
probabilities' on page 2
2025-01-19 09:11:13,518 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,518 - src.functions - WARNING - Could not find sentence 'These measure the signed distance between each instance and the decision boundary:
Unlike 
, 
 doesn’t have a 
 method to estimate the class
probabilities' on page 2.
2025-01-19 09:11:13,526 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-19 09:11:13,526 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,526 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-19 09:11:13,535 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3
2025-01-19 09:11:13,535 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,535 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3.
2025-01-19 09:11:13,543 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3
2025-01-19 09:11:13,543 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,543 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3.
2025-01-19 09:11:13,552 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 3
2025-01-19 09:11:13,552 - src.functions - DEBUG - Found rects: [Rect(75.328125, 292.1767883300781, 140.65020751953125, 308.00177001953125), Rect(144.58544921875, 292.1767883300781, 175.27883911132812, 308.00177001953125), Rect(179.214111328125, 292.1767883300781, 271.3088073730469, 308.00177001953125), Rect(75.328125, 314.5888671875, 534.448974609375, 325.6630859375), Rect(75.328125, 327.0888671875, 170.83660888671875, 338.1630859375)]
2025-01-19 09:11:13,567 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 3
2025-01-19 09:11:13,567 - src.functions - DEBUG - Found rects: [Rect(175.83595275878906, 327.0888671875, 529.6141357421875, 338.1630859375), Rect(75.328125, 339.5888671875, 444.8943786621094, 350.6630859375)]
2025-01-19 09:11:13,578 - src.functions - DEBUG - Searching for sentence #44: 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 3
2025-01-19 09:11:13,578 - src.functions - DEBUG - Found rects: [Rect(244.54690551757812, 481.816650390625, 403.197998046875, 490.122314453125), Rect(75.328125, 499.5888671875, 402.28619384765625, 510.6630859375), Rect(75.328125, 513.0888671875, 346.64215087890625, 524.1630859375), Rect(416.6328125, 513.0888671875, 443.0087890625, 524.1630859375), Rect(488.0, 513.0888671875, 526.592041015625, 524.1630859375)]
2025-01-19 09:11:13,591 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3
2025-01-19 09:11:13,592 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,592 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3.
2025-01-19 09:11:13,600 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3
2025-01-19 09:11:13,600 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,600 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3.
2025-01-19 09:11:13,609 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3
2025-01-19 09:11:13,609 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,609 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3.
2025-01-19 09:11:13,618 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 3
2025-01-19 09:11:13,618 - src.functions - DEBUG - Found rects: [Rect(75.328125, 525.5888671875, 521.5821533203125, 536.6630859375), Rect(75.328125, 539.0888671875, 271.3769836425781, 550.1630859375)]
2025-01-19 09:11:13,629 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-19 09:11:13,629 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,629 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-19 09:11:13,637 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3
2025-01-19 09:11:13,637 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,637 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3.
2025-01-19 09:11:13,646 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3
2025-01-19 09:11:13,646 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,646 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3.
2025-01-19 09:11:13,655 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-19 09:11:13,655 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,655 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-19 09:11:13,663 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 3
2025-01-19 09:11:13,663 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,663 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 3.
2025-01-19 09:11:13,672 - src.functions - DEBUG - Searching for sentence #22: 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 3
2025-01-19 09:11:13,672 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,672 - src.functions - WARNING - Could not find sentence 'The objective is to find a good balance between
keeping the street as large as possible and limiting the margin violations (i' on page 3.
2025-01-19 09:11:13,681 - src.functions - DEBUG - Searching for sentence #27: 'As you can see, reducing  makes the street larger,
but it also leads to more margin violations' on page 3
2025-01-19 09:11:13,681 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,681 - src.functions - WARNING - Could not find sentence 'As you can see, reducing  makes the street larger,
but it also leads to more margin violations' on page 3.
2025-01-19 09:11:13,689 - src.functions - DEBUG - Searching for sentence #28: 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 3
2025-01-19 09:11:13,689 - src.functions - DEBUG - Found rects: []
2025-01-19 09:11:13,689 - src.functions - WARNING - Could not find sentence 'In other words, reducing  results in more instances supporting the
street, so there’s less risk of overfitting' on page 3.
2025-01-19 09:11:13,698 - src.functions - DEBUG - Searching for sentence #36: 'These measure the signed distance between each instance and the decision boundary:
Unlike 
, 
 doesn’t have a 
 method to estimate the class
probabilities' on page 3
2025-01-19 09:11:13,698 - src.functions - DEBUG - Found rects: [Rect(182.23812866210938, 124.5888671875, 522.9282836914062, 135.6630859375), Rect(75.328125, 181.0888671875, 105.04412841796875, 192.1630859375), Rect(195.03125, 181.0888671875, 200.03125, 192.1630859375), Rect(245.02345275878906, 181.0888671875, 307.594970703125, 192.1630859375), Rect(382.5859375, 181.0888671875, 497.8293762207031, 192.1630859375), Rect(75.328125, 194.5888671875, 125.31572723388672, 205.6630859375)]
2025-01-19 09:22:25,422 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:22:25,423 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:22:25,423 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:22:25,423 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:22:25,423 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:22:25,423 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:22:25,482 - src.functions - DEBUG - Sentences: ['Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', ', hundreds to thousands of instances), especially for classification tasks', 'However, they\ndon’t scale very well to very large datasets, as you will see', 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 'The left plot shows the decision boundaries of three possible linear classifiers', 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 'First, it only works if the data is linearly\nseparable', 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', ', instances that end up in the middle of\nthe street or even on the wrong side)', 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 'With a high value, you get the model on the right', 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 'Then, as usual, you can use the model to make predictions:', 'The first plant is classified as an Iris virginica, while the second is not', 'Let’s look at the scores that the SVM used\nto make these predictions', 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 'This dataset is not linearly separable, as\nyou can see', 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 'You can generate this dataset using the \n function:\n1\n2\n1\n2']
2025-01-19 09:22:25,482 - src.functions - DEBUG - Numbered Sentences: {1: 'Support Vector Machines\nA support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear\nor nonlinear classification, regression, and even novelty detection', 2: ', hundreds to thousands of instances), especially for classification tasks', 3: 'However, they\ndon’t scale very well to very large datasets, as you will see', 4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work', 5: 'Let’s jump right in!\nLinear SVM Classification\nThe fundamental idea behind SVMs is best explained with some visuals', 6: 'Figure 5-1 shows part of the iris dataset\nthat was introduced at the end of Chapter 4', 7: 'The two classes can clearly be separated easily with a straight line\n(they are linearly separable)', 8: 'The left plot shows the decision boundaries of three possible linear classifiers', 9: 'The\nmodel whose decision boundary is represented by the dashed line is so bad that it does not even separate the\nclasses properly', 10: 'The other two models work perfectly on this training set, but their decision boundaries come so\nclose to the instances that these models will probably not perform as well on new instances', 11: 'In contrast, the solid\nline in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the\ntwo classes but also stays as far away from the closest training instances as possible', 12: 'You can think of an SVM\nclassifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes', 13: 'Large margin classification\nNotice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully\ndetermined (or “supported”) by the instances located on the edge of the street', 14: 'These instances are called the\nsupport vectors (they are circled in Figure 5-1)', 15: 'WARNING\nSVMs are sensitive to the feature scales, as you can see in Figure 5-2', 16: 'In the left plot, the vertical scale is much larger than the horizontal\nscale, so the widest possible street is close to horizontal', 17: ', using Scikit-Learn’s \n), the decision\nboundary in the right plot looks much better', 18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin\nclassification', 19: 'First, it only works if the data is linearly\nseparable', 20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on\nthe left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the\none we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well', 21: 'Hard margin sensitivity to outliers\nTo avoid these issues, we need to use a more flexible model', 22: 'The objective is to find a good balance between\nkeeping the street as large as possible and limiting the margin violations (i', 23: ', instances that end up in the middle of\nthe street or even on the wrong side)', 24: 'When creating an SVM model using Scikit-Learn, you can specify several hyperparameters, including the\nregularization hyperparameter', 25: 'If you set it to a low value, then you end up with the model on the left of\nFigure 5-4', 26: 'With a high value, you get the model on the right', 27: 'As you can see, reducing  makes the street larger,\nbut it also leads to more margin violations', 28: 'In other words, reducing  results in more instances supporting the\nstreet, so there’s less risk of overfitting', 29: 'But if you reduce it too much, then the model ends up underfitting, as\nseems to be the case here: the model with \n looks like it will generalize better than the one with', 30: 'Large margin (left) versus fewer margin violations (right)\nTIP\nIf your SVM model is overfitting, you can try regularizing it by reducing', 31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica\nflowers', 32: 'The pipeline first scales the features, then uses a \n with \n:\nThe resulting model is represented on the left in Figure 5-4', 33: 'Then, as usual, you can use the model to make predictions:', 34: 'The first plant is classified as an Iris virginica, while the second is not', 35: 'Let’s look at the scores that the SVM used\nto make these predictions', 36: 'These measure the signed distance between each instance and the decision boundary:\nUnlike \n, \n doesn’t have a \n method to estimate the class\nprobabilities', 37: 'That said, if you use the \n class (discussed shortly) instead of \n, and if you set its\n hyperparameter to \n, then the model will fit an extra model at the end of training to map the\nSVM decision function scores to estimated probabilities', 38: 'Under the hood, this requires using 5-fold cross-\nvalidation to generate out-of-sample predictions for every instance in the training set, then training a\n model, so it will slow down training considerably', 39: 'Nonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to\nbeing linearly separable', 40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial\nfeatures (as we did in Chapter 4); in some cases this can result in a linearly separable dataset', 41: 'Consider the lefthand\nplot in Figure 5-5: it represents a simple dataset with just one feature, x', 42: 'This dataset is not linearly separable, as\nyou can see', 43: 'But if you add a second feature x  = (x ) , the resulting 2D dataset is perfectly linearly separable', 44: 'Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, you can create a pipeline containing a \ntransformer (discussed in “Polynomial Regression”), followed by a \n and a \n classifier', 45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as\ntwo interleaving crescent moons (see Figure 5-6)', 46: 'You can generate this dataset using the \n function:\n1\n2\n1\n2'}
2025-01-19 09:22:25,710 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-19 09:22:28,607 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-19 09:22:28,610 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    18,
    39
  ],
  "Examples": [
    6,
    20,
    31,
    45
  ],
  "Definitions": [
    1,
    12,
    14,
    15,
    19,
    40,
    44
  ]
}
```
2025-01-19 09:22:28,621 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-19 09:22:28,622 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-19 09:22:28,648 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 1
2025-01-19 09:22:28,648 - src.functions - DEBUG - Found rects: [Rect(75.328125, 182.0888671875, 438.5721130371094, 193.1630859375)]
2025-01-19 09:22:28,656 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 1
2025-01-19 09:22:28,656 - src.functions - DEBUG - Found rects: [Rect(443.5721130371094, 182.0888671875, 521.6256103515625, 193.1630859375), Rect(75.328125, 214.1768035888672, 117.83416748046875, 230.00177001953125), Rect(121.76940155029297, 214.1768035888672, 152.46279907226562, 230.00177001953125), Rect(156.39804077148438, 214.1768035888672, 248.49273681640625, 230.00177001953125), Rect(75.328125, 236.5888671875, 363.6017150878906, 247.6630859375)]
2025-01-19 09:22:28,670 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1
2025-01-19 09:22:28,670 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,670 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 1.
2025-01-19 09:22:28,674 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1
2025-01-19 09:22:28,674 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,674 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 1.
2025-01-19 09:22:28,677 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 1
2025-01-19 09:22:28,678 - src.functions - DEBUG - Found rects: [Rect(368.6094055175781, 236.5888671875, 527.1851806640625, 247.6630859375), Rect(75.328125, 249.0888671875, 247.49119567871094, 260.1630859375)]
2025-01-19 09:22:28,684 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1
2025-01-19 09:22:28,684 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,684 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 1.
2025-01-19 09:22:28,687 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1
2025-01-19 09:22:28,687 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,687 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 1.
2025-01-19 09:22:28,690 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1
2025-01-19 09:22:28,690 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,690 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 1.
2025-01-19 09:22:28,693 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 1
2025-01-19 09:22:28,693 - src.functions - DEBUG - Found rects: [Rect(178.6875, 72.89453125, 255.33551025390625, 95.23828125), Rect(260.8915100097656, 72.89453125, 322.038818359375, 95.23828125), Rect(327.5948486328125, 72.89453125, 418.73785400390625, 95.23828125), Rect(75.328125, 127.0888671875, 536.5504760742188, 138.1630859375), Rect(75.328125, 139.5888671875, 337.73016357421875, 150.6630859375)]
2025-01-19 09:22:28,705 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 1
2025-01-19 09:22:28,705 - src.functions - DEBUG - Found rects: [Rect(415.2421875, 324.0888671875, 517.8462524414062, 335.1630859375), Rect(75.328125, 336.5888671875, 499.3355712890625, 347.6630859375)]
2025-01-19 09:22:28,713 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 1
2025-01-19 09:22:28,713 - src.functions - DEBUG - Found rects: [Rect(389.6484375, 468.5888671875, 508.47076416015625, 479.6630859375), Rect(75.328125, 481.0888671875, 262.767578125, 492.1630859375)]
2025-01-19 09:22:28,721 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 1
2025-01-19 09:22:28,721 - src.functions - DEBUG - Found rects: [Rect(280.1015625, 515.4517822265625, 331.3959655761719, 526.6181030273438), Rect(94.828125, 532.816650390625, 302.26629638671875, 541.122314453125)]
2025-01-19 09:22:28,727 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 1
2025-01-19 09:22:28,727 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,727 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 1.
2025-01-19 09:22:28,730 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1
2025-01-19 09:22:28,730 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,730 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 1.
2025-01-19 09:22:28,733 - src.functions - DEBUG - Searching for sentence #44: 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 1
2025-01-19 09:22:28,733 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,733 - src.functions - WARNING - Could not find sentence 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 1.
2025-01-19 09:22:28,744 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-19 09:22:28,745 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,745 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-19 09:22:28,756 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2
2025-01-19 09:22:28,756 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,756 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 2.
2025-01-19 09:22:28,767 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2
2025-01-19 09:22:28,767 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,767 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 2.
2025-01-19 09:22:28,778 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 2
2025-01-19 09:22:28,778 - src.functions - DEBUG - Found rects: [Rect(75.328125, 72.5888671875, 513.7359619140625, 83.6630859375), Rect(75.328125, 85.0888671875, 128.6561279296875, 96.1630859375)]
2025-01-19 09:22:28,793 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2
2025-01-19 09:22:28,793 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,793 - src.functions - WARNING - Could not find sentence 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 2.
2025-01-19 09:22:28,804 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2
2025-01-19 09:22:28,804 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,804 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 2.
2025-01-19 09:22:28,816 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 2
2025-01-19 09:22:28,816 - src.functions - DEBUG - Found rects: [Rect(252.51565551757812, 97.5888671875, 522.7468872070312, 108.6630859375), Rect(75.328125, 110.0888671875, 527.3569946289062, 121.1630859375), Rect(75.328125, 122.5888671875, 459.814208984375, 133.6630859375)]
2025-01-19 09:22:28,833 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 2
2025-01-19 09:22:28,833 - src.functions - DEBUG - Found rects: [Rect(75.328125, 522.0888671875, 517.880615234375, 533.1630859375), Rect(75.328125, 535.5888671875, 105.3177261352539, 546.6630859375)]
2025-01-19 09:22:28,848 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2
2025-01-19 09:22:28,848 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,848 - src.functions - WARNING - Could not find sentence 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 2.
2025-01-19 09:22:28,859 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2
2025-01-19 09:22:28,859 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,859 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 2.
2025-01-19 09:22:28,870 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2
2025-01-19 09:22:28,870 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,870 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 2.
2025-01-19 09:22:28,881 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2
2025-01-19 09:22:28,881 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,881 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 2.
2025-01-19 09:22:28,892 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 2
2025-01-19 09:22:28,892 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,892 - src.functions - WARNING - Could not find sentence 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 2.
2025-01-19 09:22:28,904 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 2
2025-01-19 09:22:28,904 - src.functions - DEBUG - Found rects: [Rect(369.5124206542969, 85.0888671875, 530.3182373046875, 96.1630859375), Rect(75.328125, 97.5888671875, 113.08060455322266, 108.6630859375)]
2025-01-19 09:22:28,918 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2
2025-01-19 09:22:28,918 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,918 - src.functions - WARNING - Could not find sentence 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 2.
2025-01-19 09:22:28,929 - src.functions - DEBUG - Searching for sentence #44: 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 2
2025-01-19 09:22:28,929 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,929 - src.functions - WARNING - Could not find sentence 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 2.
2025-01-19 09:22:28,943 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-19 09:22:28,943 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,943 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-19 09:22:28,957 - src.functions - DEBUG - Searching for sentence #4: 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3
2025-01-19 09:22:28,957 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,957 - src.functions - WARNING - Could not find sentence 'This chapter will explain the core concepts of SVMs, how to use them, and how they work' on page 3.
2025-01-19 09:22:28,971 - src.functions - DEBUG - Searching for sentence #5: 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3
2025-01-19 09:22:28,971 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,971 - src.functions - WARNING - Could not find sentence 'Let’s jump right in!
Linear SVM Classification
The fundamental idea behind SVMs is best explained with some visuals' on page 3.
2025-01-19 09:22:28,985 - src.functions - DEBUG - Searching for sentence #18: 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3
2025-01-19 09:22:28,985 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:28,985 - src.functions - WARNING - Could not find sentence 'If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin
classification' on page 3.
2025-01-19 09:22:29,000 - src.functions - DEBUG - Searching for sentence #39: 'Nonlinear SVM Classification
Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to
being linearly separable' on page 3
2025-01-19 09:22:29,000 - src.functions - DEBUG - Found rects: [Rect(75.328125, 292.1767883300781, 140.65020751953125, 308.00177001953125), Rect(144.58544921875, 292.1767883300781, 175.27883911132812, 308.00177001953125), Rect(179.214111328125, 292.1767883300781, 271.3088073730469, 308.00177001953125), Rect(75.328125, 314.5888671875, 534.448974609375, 325.6630859375), Rect(75.328125, 327.0888671875, 170.83660888671875, 338.1630859375)]
2025-01-19 09:22:29,021 - src.functions - DEBUG - Searching for sentence #6: 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3
2025-01-19 09:22:29,021 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:29,021 - src.functions - WARNING - Could not find sentence 'Figure 5-1 shows part of the iris dataset
that was introduced at the end of Chapter 4' on page 3.
2025-01-19 09:22:29,035 - src.functions - DEBUG - Searching for sentence #20: 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3
2025-01-19 09:22:29,035 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:29,035 - src.functions - WARNING - Could not find sentence 'Figure 5-3 shows the iris dataset with just one additional outlier: on
the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the
one we saw in Figure 5-1 without the outlier, and the model will probably not generalize as well' on page 3.
2025-01-19 09:22:29,050 - src.functions - DEBUG - Searching for sentence #31: 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3
2025-01-19 09:22:29,050 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:29,050 - src.functions - WARNING - Could not find sentence 'The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica
flowers' on page 3.
2025-01-19 09:22:29,066 - src.functions - DEBUG - Searching for sentence #45: 'Let’s test this on the moons dataset, a toy dataset for binary classification in which the data points are shaped as
two interleaving crescent moons (see Figure 5-6)' on page 3
2025-01-19 09:22:29,066 - src.functions - DEBUG - Found rects: [Rect(75.328125, 525.5888671875, 521.5821533203125, 536.6630859375), Rect(75.328125, 539.0888671875, 271.3769836425781, 550.1630859375)]
2025-01-19 09:22:29,084 - src.functions - DEBUG - Searching for sentence #1: 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3
2025-01-19 09:22:29,084 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:29,084 - src.functions - WARNING - Could not find sentence 'Support Vector Machines
A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear
or nonlinear classification, regression, and even novelty detection' on page 3.
2025-01-19 09:22:29,098 - src.functions - DEBUG - Searching for sentence #12: 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3
2025-01-19 09:22:29,098 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:29,098 - src.functions - WARNING - Could not find sentence 'You can think of an SVM
classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes' on page 3.
2025-01-19 09:22:29,112 - src.functions - DEBUG - Searching for sentence #14: 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3
2025-01-19 09:22:29,112 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:29,113 - src.functions - WARNING - Could not find sentence 'These instances are called the
support vectors (they are circled in Figure 5-1)' on page 3.
2025-01-19 09:22:29,127 - src.functions - DEBUG - Searching for sentence #15: 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 3
2025-01-19 09:22:29,127 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:29,127 - src.functions - WARNING - Could not find sentence 'WARNING
SVMs are sensitive to the feature scales, as you can see in Figure 5-2' on page 3.
2025-01-19 09:22:29,141 - src.functions - DEBUG - Searching for sentence #19: 'First, it only works if the data is linearly
separable' on page 3
2025-01-19 09:22:29,141 - src.functions - DEBUG - Found rects: []
2025-01-19 09:22:29,141 - src.functions - WARNING - Could not find sentence 'First, it only works if the data is linearly
separable' on page 3.
2025-01-19 09:22:29,155 - src.functions - DEBUG - Searching for sentence #40: 'One approach to handling nonlinear datasets is to add more features, such as polynomial
features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset' on page 3
2025-01-19 09:22:29,155 - src.functions - DEBUG - Found rects: [Rect(175.83595275878906, 327.0888671875, 529.6141357421875, 338.1630859375), Rect(75.328125, 339.5888671875, 444.8943786621094, 350.6630859375)]
2025-01-19 09:22:29,174 - src.functions - DEBUG - Searching for sentence #44: 'Adding features to make a dataset linearly separable
To implement this idea using Scikit-Learn, you can create a pipeline containing a 
transformer (discussed in “Polynomial Regression”), followed by a 
 and a 
 classifier' on page 3
2025-01-19 09:22:29,175 - src.functions - DEBUG - Found rects: [Rect(244.54690551757812, 481.816650390625, 403.197998046875, 490.122314453125), Rect(75.328125, 499.5888671875, 402.28619384765625, 510.6630859375), Rect(75.328125, 513.0888671875, 346.64215087890625, 524.1630859375), Rect(416.6328125, 513.0888671875, 443.0087890625, 524.1630859375), Rect(488.0, 513.0888671875, 526.592041015625, 524.1630859375)]
2025-01-19 09:27:59,958 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:27:59,958 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:27:59,958 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:27:59,958 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:27:59,958 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:27:59,958 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:27:59,958 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:27:59,958 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:27:59,958 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:27:59,958 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:27:59,959 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:27:59,959 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:28:04,567 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:28:04,567 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:28:04,568 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:28:04,568 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:28:04,568 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:28:04,568 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:28:07,248 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:28:07,249 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:28:07,249 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:28:07,249 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:28:07,249 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:28:07,249 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:28:14,519 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:28:14,519 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:28:14,519 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:28:14,520 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:28:14,520 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:28:14,520 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:28:14,619 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-19 09:28:22,298 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-19 09:28:22,301 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    11,
    18,
    39
  ],
  "Examples": [
    6,
    10,
    20,
    31,
    41,
    45
  ],
  "Definitions": [
    1,
    12,
    14,
    15,
    19,
    28
  ]
}
```
2025-01-19 09:28:22,301 - src.functions - ERROR - Failed to decode JSON from ChatGPT response: Expecting value: line 1 column 1 (char 0)
2025-01-19 09:28:22,302 - src.functions - ERROR - No categorized sentences returned from ChatGPT.
2025-01-19 09:28:22,304 - __main__ - ERROR - Error generating highlighted PDF: Error processing the PDF with AI.
2025-01-19 09:29:49,301 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:29:49,301 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:29:49,301 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:29:49,301 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:29:49,302 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:29:49,302 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:29:49,357 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:29:49,357 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:29:49,358 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:29:49,358 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:29:49,358 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:29:49,358 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:29:49,462 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-19 09:29:52,110 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-19 09:29:52,113 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    39
  ],
  "Examples": [
    6,
    20,
    31,
    45
  ],
  "Definitions": [
    1,
    12,
    14,
    18
  ]
}
```
2025-01-19 09:29:52,115 - src.functions - ERROR - Error generating highlighted PDF: name 're' is not defined
2025-01-19 09:29:52,116 - __main__ - ERROR - Error generating highlighted PDF: An error occurred while processing the PDF.
2025-01-19 09:30:13,337 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:30:13,338 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:30:13,338 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:30:13,338 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:30:13,338 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:30:13,339 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:30:13,387 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:30:13,387 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:30:13,387 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:30:13,387 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:30:13,387 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:30:13,387 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:30:13,447 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-19 09:30:16,375 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-19 09:30:16,377 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    11,
    18,
    21,
    39
  ],
  "Examples": [
    6,
    20,
    31,
    34,
    45
  ],
  "Definitions": [
    1,
    12,
    14,
    15,
    19,
    40,
    44
  ]
}
```
2025-01-19 09:30:21,072 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:30:21,073 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:30:21,073 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:30:21,073 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:30:21,073 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:30:21,073 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:30:21,151 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-19 09:30:23,851 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-19 09:30:23,852 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    11,
    18,
    39
  ],
  "Examples": [
    6,
    20,
    31,
    45
  ],
  "Definitions": [
    1,
    12,
    14,
    15,
    19,
    22,
    28,
    40,
     44
  ]
}
```
2025-01-19 09:43:13,059 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:43:13,062 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:43:13,062 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:43:13,063 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:43:13,063 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:43:13,064 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:43:13,079 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:43:13,868 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:43:13,909 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:43:13,909 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:43:13,909 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:43:13,910 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:43:14,384 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-19 09:43:18,173 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-19 09:43:18,175 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    39
  ],
  "Examples": [
    6,
    20,
    31,
    45
  ],
  "Definitions": [
    1,
    12,
    13,
    18,
    19,
    28
  ]
}
```
2025-01-19 09:46:24,705 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:46:24,706 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:46:24,706 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:46:24,706 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:46:24,706 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:46:24,706 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:46:24,785 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:46:24,785 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:46:24,786 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:46:24,786 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 09:46:24,786 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 09:46:24,786 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 09:46:25,025 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-19 09:46:28,837 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-19 09:46:28,839 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    11,
    18,
    39
  ],
  "Examples": [
    6,
    20,
    31,
    45
  ],
  "Definitions": [
    1,
    12,
    14,
    15,
    19,
    28
  ]
}
```
2025-01-19 10:32:01,888 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 10:32:01,891 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 10:32:01,891 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 10:32:01,891 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 10:32:01,891 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 10:32:01,891 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 10:32:01,897 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 10:32:01,898 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 10:32:01,898 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 10:32:01,898 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-01-19 10:32:01,899 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-01-19 10:32:01,899 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 691
2025-01-19 10:32:03,037 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.openai.com:443
2025-01-19 10:33:11,220 - urllib3.connectionpool - DEBUG - https://api.openai.com:443 "POST /v1/chat/completions HTTP/11" 200 None
2025-01-19 10:33:11,223 - src.functions - DEBUG - ChatGPT Output: ```json
{
  "Key Concepts": [
    1,
    4,
    5,
    18,
    39,
     40
  ],
  "Examples": [
    6,
    20,
    31,
    34,
    45
  ],
  "Definitions": [
    1,
    12,
    14,
    15,
    19,
    28
  ]
}
```
